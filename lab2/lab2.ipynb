{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed a project of Machine Learning with spark ML, in this assignment, we will be swithing to the context of Deep Learning with Tensorflow/Keras by two tasks:\n",
    "- Task1: Image Classification with CNN\n",
    "- Task2: Image captioning with a combination of CNN and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Going Deeper with convolutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before **Inception v1** (**GoogLeNet**), which is the winner of the **ILSVRC** (ImageNet Large Scale Visual Recognition Competition) in 2014, most popular CNNs just stacked convolution layers deeper and deeper, hoping to get better performance.\n",
    "The Inception network, however, uses a lot of tricks to improve performance in terms of speed and accuracy.\n",
    "Compared to other networks, **Inception v1** has significant improvement over **ZFNet** (the winner in 2013) and **AlexNet** (the winner in 2012), and has relatively lower error rate compared with the VGGNet.\n",
    "\n",
    "In this task, we will be implementing the inception architecture [in this paper](https://arxiv.org/abs/1409.4842) with TensorFlow/Keras/Pytorch. \n",
    "\n",
    "The goal of this task is to understand how to write code to build the model, as long as you can verify the correctness of the code (e.g., through Keras model summary), it is not necessary to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/mrobakowski/.conda/lib/python3.7/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy in /home/mrobakowski/.conda/lib/python3.7/site-packages (from torch) (1.17.4)\n",
      "Requirement already satisfied: torchsummary in /home/mrobakowski/.conda/lib/python3.7/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "# conda env torch_planet\n",
    "!pip install torch\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspired from https://github.com/pytorch/vision/blob/master/torchvision/models/googlenet.py\n",
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.jit.annotations import Optional, Tuple\n",
    "from torch import Tensor\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GoogLeNetOutputs = namedtuple('GoogLeNetOutputs', ['logits', 'aux_logits2', 'aux_logits1'])\n",
    "GoogLeNetOutputs.__annotations__ = {'logits': Tensor, 'aux_logits2': Optional[Tensor],\n",
    "                                    'aux_logits1': Optional[Tensor]}\n",
    "\n",
    "# _GoogLeNetOutputs set here for backwards compat\n",
    "_GoogLeNetOutputs = GoogLeNetOutputs\n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000, aux_logits=True, blocks=None):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        if blocks is None:\n",
    "            blocks = [BasicConv2d, Inception, InceptionAux]\n",
    "        assert len(blocks) == 3\n",
    "        conv_block = blocks[0]\n",
    "        inception_block = blocks[1]\n",
    "        inception_aux_block = blocks[2]\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "        self.conv2 = conv_block(64, 64, kernel_size=1)\n",
    "        self.conv3 = conv_block(64, 192, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = inception_block(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = inception_block(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception4a = inception_block(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = inception_block(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = inception_block(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = inception_block(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = inception_block(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception5a = inception_block(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = inception_block(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        if aux_logits:\n",
    "            self.aux1 = inception_aux_block(512, num_classes)\n",
    "            self.aux2 = inception_aux_block(528, num_classes)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                X = stats.truncnorm(-2, 2, scale=0.01)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def _forward(self, x):\n",
    "        # type: (Tensor) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]\n",
    "        # N x 3 x 224 x 224\n",
    "        x = self.conv1(x)\n",
    "        # N x 64 x 112 x 112\n",
    "        x = self.maxpool1(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv2(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv3(x)\n",
    "        # N x 192 x 56 x 56\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # N x 192 x 28 x 28\n",
    "        x = self.inception3a(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        x = self.inception3b(x)\n",
    "        # N x 480 x 28 x 28\n",
    "        x = self.maxpool3(x)\n",
    "        # N x 480 x 14 x 14\n",
    "        x = self.inception4a(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if aux_defined:\n",
    "            aux1 = self.aux1(x)\n",
    "        else:\n",
    "            aux1 = None\n",
    "\n",
    "        x = self.inception4b(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4c(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4d(x)\n",
    "        # N x 528 x 14 x 14\n",
    "        if aux_defined:\n",
    "            aux2 = self.aux2(x)\n",
    "        else:\n",
    "            aux2 = None\n",
    "\n",
    "        x = self.inception4e(x)\n",
    "        # N x 832 x 14 x 14\n",
    "        x = self.maxpool4(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5a(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5b(x)\n",
    "        # N x 1024 x 7 x 7\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # N x 1024 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        return x, aux2, aux1\n",
    "\n",
    "#     @torch.jit.unused\n",
    "    def eager_outputs(self, x, aux2, aux1):\n",
    "        # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> GoogLeNetOutputs\n",
    "        if self.training and self.aux_logits:\n",
    "            return _GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # type: (Tensor) -> GoogLeNetOutputs\n",
    "        x, aux1, aux2 = self._forward(x)\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if torch.jit.is_scripting():\n",
    "            if not aux_defined:\n",
    "                warnings.warn(\"Scripted GoogleNet always returns GoogleNetOutputs Tuple\")\n",
    "            return GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return self.eager_outputs(x, aux2, aux1)\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    __constants__ = ['branch2', 'branch3', 'branch4']\n",
    "\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj,\n",
    "                 conv_block=None):\n",
    "        super(Inception, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            conv_block(in_channels, ch3x3red, kernel_size=1),\n",
    "            conv_block(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            conv_block(in_channels, ch5x5red, kernel_size=1),\n",
    "            conv_block(ch5x5red, ch5x5, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n",
    "            conv_block(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        outputs = [branch1, branch2, branch3, branch4]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, conv_block=None):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        # N x 1024\n",
    "        x = F.dropout(x, 0.7, training=self.training)\n",
    "        # N x 1024\n",
    "        x = self.fc2(x)\n",
    "        # N x 1000 (num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= GoogLeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "       BasicConv2d-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "       BasicConv2d-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8          [-1, 192, 56, 56]         110,592\n",
      "       BatchNorm2d-9          [-1, 192, 56, 56]             384\n",
      "      BasicConv2d-10          [-1, 192, 56, 56]               0\n",
      "        MaxPool2d-11          [-1, 192, 28, 28]               0\n",
      "           Conv2d-12           [-1, 64, 28, 28]          12,288\n",
      "      BatchNorm2d-13           [-1, 64, 28, 28]             128\n",
      "      BasicConv2d-14           [-1, 64, 28, 28]               0\n",
      "           Conv2d-15           [-1, 96, 28, 28]          18,432\n",
      "      BatchNorm2d-16           [-1, 96, 28, 28]             192\n",
      "      BasicConv2d-17           [-1, 96, 28, 28]               0\n",
      "           Conv2d-18          [-1, 128, 28, 28]         110,592\n",
      "      BatchNorm2d-19          [-1, 128, 28, 28]             256\n",
      "      BasicConv2d-20          [-1, 128, 28, 28]               0\n",
      "           Conv2d-21           [-1, 16, 28, 28]           3,072\n",
      "      BatchNorm2d-22           [-1, 16, 28, 28]              32\n",
      "      BasicConv2d-23           [-1, 16, 28, 28]               0\n",
      "           Conv2d-24           [-1, 32, 28, 28]           4,608\n",
      "      BatchNorm2d-25           [-1, 32, 28, 28]              64\n",
      "      BasicConv2d-26           [-1, 32, 28, 28]               0\n",
      "        MaxPool2d-27          [-1, 192, 28, 28]               0\n",
      "           Conv2d-28           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-29           [-1, 32, 28, 28]              64\n",
      "      BasicConv2d-30           [-1, 32, 28, 28]               0\n",
      "        Inception-31          [-1, 256, 28, 28]               0\n",
      "           Conv2d-32          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-33          [-1, 128, 28, 28]             256\n",
      "      BasicConv2d-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "      BasicConv2d-37          [-1, 128, 28, 28]               0\n",
      "           Conv2d-38          [-1, 192, 28, 28]         221,184\n",
      "      BatchNorm2d-39          [-1, 192, 28, 28]             384\n",
      "      BasicConv2d-40          [-1, 192, 28, 28]               0\n",
      "           Conv2d-41           [-1, 32, 28, 28]           8,192\n",
      "      BatchNorm2d-42           [-1, 32, 28, 28]              64\n",
      "      BasicConv2d-43           [-1, 32, 28, 28]               0\n",
      "           Conv2d-44           [-1, 96, 28, 28]          27,648\n",
      "      BatchNorm2d-45           [-1, 96, 28, 28]             192\n",
      "      BasicConv2d-46           [-1, 96, 28, 28]               0\n",
      "        MaxPool2d-47          [-1, 256, 28, 28]               0\n",
      "           Conv2d-48           [-1, 64, 28, 28]          16,384\n",
      "      BatchNorm2d-49           [-1, 64, 28, 28]             128\n",
      "      BasicConv2d-50           [-1, 64, 28, 28]               0\n",
      "        Inception-51          [-1, 480, 28, 28]               0\n",
      "        MaxPool2d-52          [-1, 480, 14, 14]               0\n",
      "           Conv2d-53          [-1, 192, 14, 14]          92,160\n",
      "      BatchNorm2d-54          [-1, 192, 14, 14]             384\n",
      "      BasicConv2d-55          [-1, 192, 14, 14]               0\n",
      "           Conv2d-56           [-1, 96, 14, 14]          46,080\n",
      "      BatchNorm2d-57           [-1, 96, 14, 14]             192\n",
      "      BasicConv2d-58           [-1, 96, 14, 14]               0\n",
      "           Conv2d-59          [-1, 208, 14, 14]         179,712\n",
      "      BatchNorm2d-60          [-1, 208, 14, 14]             416\n",
      "      BasicConv2d-61          [-1, 208, 14, 14]               0\n",
      "           Conv2d-62           [-1, 16, 14, 14]           7,680\n",
      "      BatchNorm2d-63           [-1, 16, 14, 14]              32\n",
      "      BasicConv2d-64           [-1, 16, 14, 14]               0\n",
      "           Conv2d-65           [-1, 48, 14, 14]           6,912\n",
      "      BatchNorm2d-66           [-1, 48, 14, 14]              96\n",
      "      BasicConv2d-67           [-1, 48, 14, 14]               0\n",
      "        MaxPool2d-68          [-1, 480, 14, 14]               0\n",
      "           Conv2d-69           [-1, 64, 14, 14]          30,720\n",
      "      BatchNorm2d-70           [-1, 64, 14, 14]             128\n",
      "      BasicConv2d-71           [-1, 64, 14, 14]               0\n",
      "        Inception-72          [-1, 512, 14, 14]               0\n",
      "           Conv2d-73            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-74            [-1, 128, 4, 4]             256\n",
      "      BasicConv2d-75            [-1, 128, 4, 4]               0\n",
      "           Linear-76                 [-1, 1024]       2,098,176\n",
      "           Linear-77                 [-1, 1000]       1,025,000\n",
      "     InceptionAux-78                 [-1, 1000]               0\n",
      "           Conv2d-79          [-1, 160, 14, 14]          81,920\n",
      "      BatchNorm2d-80          [-1, 160, 14, 14]             320\n",
      "      BasicConv2d-81          [-1, 160, 14, 14]               0\n",
      "           Conv2d-82          [-1, 112, 14, 14]          57,344\n",
      "      BatchNorm2d-83          [-1, 112, 14, 14]             224\n",
      "      BasicConv2d-84          [-1, 112, 14, 14]               0\n",
      "           Conv2d-85          [-1, 224, 14, 14]         225,792\n",
      "      BatchNorm2d-86          [-1, 224, 14, 14]             448\n",
      "      BasicConv2d-87          [-1, 224, 14, 14]               0\n",
      "           Conv2d-88           [-1, 24, 14, 14]          12,288\n",
      "      BatchNorm2d-89           [-1, 24, 14, 14]              48\n",
      "      BasicConv2d-90           [-1, 24, 14, 14]               0\n",
      "           Conv2d-91           [-1, 64, 14, 14]          13,824\n",
      "      BatchNorm2d-92           [-1, 64, 14, 14]             128\n",
      "      BasicConv2d-93           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-94          [-1, 512, 14, 14]               0\n",
      "           Conv2d-95           [-1, 64, 14, 14]          32,768\n",
      "      BatchNorm2d-96           [-1, 64, 14, 14]             128\n",
      "      BasicConv2d-97           [-1, 64, 14, 14]               0\n",
      "        Inception-98          [-1, 512, 14, 14]               0\n",
      "           Conv2d-99          [-1, 128, 14, 14]          65,536\n",
      "     BatchNorm2d-100          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-101          [-1, 128, 14, 14]               0\n",
      "          Conv2d-102          [-1, 128, 14, 14]          65,536\n",
      "     BatchNorm2d-103          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-104          [-1, 128, 14, 14]               0\n",
      "          Conv2d-105          [-1, 256, 14, 14]         294,912\n",
      "     BatchNorm2d-106          [-1, 256, 14, 14]             512\n",
      "     BasicConv2d-107          [-1, 256, 14, 14]               0\n",
      "          Conv2d-108           [-1, 24, 14, 14]          12,288\n",
      "     BatchNorm2d-109           [-1, 24, 14, 14]              48\n",
      "     BasicConv2d-110           [-1, 24, 14, 14]               0\n",
      "          Conv2d-111           [-1, 64, 14, 14]          13,824\n",
      "     BatchNorm2d-112           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-113           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-114          [-1, 512, 14, 14]               0\n",
      "          Conv2d-115           [-1, 64, 14, 14]          32,768\n",
      "     BatchNorm2d-116           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-117           [-1, 64, 14, 14]               0\n",
      "       Inception-118          [-1, 512, 14, 14]               0\n",
      "          Conv2d-119          [-1, 112, 14, 14]          57,344\n",
      "     BatchNorm2d-120          [-1, 112, 14, 14]             224\n",
      "     BasicConv2d-121          [-1, 112, 14, 14]               0\n",
      "          Conv2d-122          [-1, 144, 14, 14]          73,728\n",
      "     BatchNorm2d-123          [-1, 144, 14, 14]             288\n",
      "     BasicConv2d-124          [-1, 144, 14, 14]               0\n",
      "          Conv2d-125          [-1, 288, 14, 14]         373,248\n",
      "     BatchNorm2d-126          [-1, 288, 14, 14]             576\n",
      "     BasicConv2d-127          [-1, 288, 14, 14]               0\n",
      "          Conv2d-128           [-1, 32, 14, 14]          16,384\n",
      "     BatchNorm2d-129           [-1, 32, 14, 14]              64\n",
      "     BasicConv2d-130           [-1, 32, 14, 14]               0\n",
      "          Conv2d-131           [-1, 64, 14, 14]          18,432\n",
      "     BatchNorm2d-132           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-133           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-134          [-1, 512, 14, 14]               0\n",
      "          Conv2d-135           [-1, 64, 14, 14]          32,768\n",
      "     BatchNorm2d-136           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-137           [-1, 64, 14, 14]               0\n",
      "       Inception-138          [-1, 528, 14, 14]               0\n",
      "          Conv2d-139            [-1, 128, 4, 4]          67,584\n",
      "     BatchNorm2d-140            [-1, 128, 4, 4]             256\n",
      "     BasicConv2d-141            [-1, 128, 4, 4]               0\n",
      "          Linear-142                 [-1, 1024]       2,098,176\n",
      "          Linear-143                 [-1, 1000]       1,025,000\n",
      "    InceptionAux-144                 [-1, 1000]               0\n",
      "          Conv2d-145          [-1, 256, 14, 14]         135,168\n",
      "     BatchNorm2d-146          [-1, 256, 14, 14]             512\n",
      "     BasicConv2d-147          [-1, 256, 14, 14]               0\n",
      "          Conv2d-148          [-1, 160, 14, 14]          84,480\n",
      "     BatchNorm2d-149          [-1, 160, 14, 14]             320\n",
      "     BasicConv2d-150          [-1, 160, 14, 14]               0\n",
      "          Conv2d-151          [-1, 320, 14, 14]         460,800\n",
      "     BatchNorm2d-152          [-1, 320, 14, 14]             640\n",
      "     BasicConv2d-153          [-1, 320, 14, 14]               0\n",
      "          Conv2d-154           [-1, 32, 14, 14]          16,896\n",
      "     BatchNorm2d-155           [-1, 32, 14, 14]              64\n",
      "     BasicConv2d-156           [-1, 32, 14, 14]               0\n",
      "          Conv2d-157          [-1, 128, 14, 14]          36,864\n",
      "     BatchNorm2d-158          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-159          [-1, 128, 14, 14]               0\n",
      "       MaxPool2d-160          [-1, 528, 14, 14]               0\n",
      "          Conv2d-161          [-1, 128, 14, 14]          67,584\n",
      "     BatchNorm2d-162          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-163          [-1, 128, 14, 14]               0\n",
      "       Inception-164          [-1, 832, 14, 14]               0\n",
      "       MaxPool2d-165            [-1, 832, 7, 7]               0\n",
      "          Conv2d-166            [-1, 256, 7, 7]         212,992\n",
      "     BatchNorm2d-167            [-1, 256, 7, 7]             512\n",
      "     BasicConv2d-168            [-1, 256, 7, 7]               0\n",
      "          Conv2d-169            [-1, 160, 7, 7]         133,120\n",
      "     BatchNorm2d-170            [-1, 160, 7, 7]             320\n",
      "     BasicConv2d-171            [-1, 160, 7, 7]               0\n",
      "          Conv2d-172            [-1, 320, 7, 7]         460,800\n",
      "     BatchNorm2d-173            [-1, 320, 7, 7]             640\n",
      "     BasicConv2d-174            [-1, 320, 7, 7]               0\n",
      "          Conv2d-175             [-1, 32, 7, 7]          26,624\n",
      "     BatchNorm2d-176             [-1, 32, 7, 7]              64\n",
      "     BasicConv2d-177             [-1, 32, 7, 7]               0\n",
      "          Conv2d-178            [-1, 128, 7, 7]          36,864\n",
      "     BatchNorm2d-179            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-180            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-181            [-1, 832, 7, 7]               0\n",
      "          Conv2d-182            [-1, 128, 7, 7]         106,496\n",
      "     BatchNorm2d-183            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-184            [-1, 128, 7, 7]               0\n",
      "       Inception-185            [-1, 832, 7, 7]               0\n",
      "          Conv2d-186            [-1, 384, 7, 7]         319,488\n",
      "     BatchNorm2d-187            [-1, 384, 7, 7]             768\n",
      "     BasicConv2d-188            [-1, 384, 7, 7]               0\n",
      "          Conv2d-189            [-1, 192, 7, 7]         159,744\n",
      "     BatchNorm2d-190            [-1, 192, 7, 7]             384\n",
      "     BasicConv2d-191            [-1, 192, 7, 7]               0\n",
      "          Conv2d-192            [-1, 384, 7, 7]         663,552\n",
      "     BatchNorm2d-193            [-1, 384, 7, 7]             768\n",
      "     BasicConv2d-194            [-1, 384, 7, 7]               0\n",
      "          Conv2d-195             [-1, 48, 7, 7]          39,936\n",
      "     BatchNorm2d-196             [-1, 48, 7, 7]              96\n",
      "     BasicConv2d-197             [-1, 48, 7, 7]               0\n",
      "          Conv2d-198            [-1, 128, 7, 7]          55,296\n",
      "     BatchNorm2d-199            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-200            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-201            [-1, 832, 7, 7]               0\n",
      "          Conv2d-202            [-1, 128, 7, 7]         106,496\n",
      "     BatchNorm2d-203            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-204            [-1, 128, 7, 7]               0\n",
      "       Inception-205           [-1, 1024, 7, 7]               0\n",
      "AdaptiveAvgPool2d-206           [-1, 1024, 1, 1]               0\n",
      "         Dropout-207                 [-1, 1024]               0\n",
      "          Linear-208                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 13,004,888\n",
      "Trainable params: 13,004,888\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 94.25\n",
      "Params size (MB): 49.61\n",
      "Estimated Total Size (MB): 144.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Show and Tell: A Neural Image Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically describing the content of an image is a fundamental problem in AI that connects *computer vision* and *natural language processing*.\n",
    "In this task, we will be looking into how we can use CNNs and RNNs to build an Image Caption Generator.\n",
    "\n",
    "Specifically, you will be implementing and training the model [in this paper](https://arxiv.org/abs/1411.4555) with TensorFlow/Keras on one of the datasets mentioned in the paper.\n",
    "\n",
    "To lighten the burden on training the network, you can use any pretrained network in [tf.keras.applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoder\n",
    "First we fetch a pre-trained convolutional neural network and tweak it slightly so it's not focused on classification anymore. We'll use this to encode our image and then feed that to the caption generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Network to encode an image\"\"\"\n",
    "    def __init__(self, out_dim=1000):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.resnet = models.resnet152(pretrained=True)\n",
    "        # it's pretrained, so let's make it not change\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        # the last layer of resnet is Linear that is used for classification\n",
    "        # we'll change its size, mark it as learnable, and initialize it randomly, since it's not going to be classification anymore\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, out_dim) # constructing it marks it as learnable as well\n",
    "        assert(all(p.requires_grad for p in self.resnet.fc.parameters()))\n",
    "        self.resnet.fc.weight.data.normal_(0, 0.02) # weights to a small number\n",
    "        self.resnet.fc.bias.data.fill_(0) # bias to zero\n",
    "        \n",
    "        # TODO: maybe it also makes sense to tweak the one layer before that, that is, the pooling layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Generator\n",
    "This is a recurrent neural network that is first initialized with the encoded image and then starts generating words that will be used as the generated caption. The word embeddings are pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartiallyFixedEmbedding(nn.Module): # from https://discuss.pytorch.org/t/updating-part-of-an-embedding-matrix-only-for-out-of-vocab-words/33297/4\n",
    "    \"\"\"\n",
    "    This embedding has an embedding matrix that is split into two parts: fixed and variable.\n",
    "    The fixed part is not changed in the learning process, making it ideal for pretrained vocabularies with a few extra words that we want to learn.\n",
    "    \"\"\"\n",
    "    def __init__(self, fixed_weights, num_to_learn):\n",
    "        super().__init__()\n",
    "        self.num_fixed = fixed_weights.size(0)\n",
    "        self.num_to_learn = num_to_learn\n",
    "        weight = torch.empty(self.num_fixed + num_to_learn, fixed_weights.size(1))\n",
    "        weight[:self.num_fixed] = fixed_weights\n",
    "        self.trainable_weight = nn.Parameter(torch.empty(num_to_learn, fixed_weights.size(1)))\n",
    "        nn.init.kaiming_uniform_(self.trainable_weight)\n",
    "        weight[self.num_fixed:] = self.trainable_weight\n",
    "        self.register_buffer('weight', weight)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        self.weight.detach_()\n",
    "        self.weight[self.num_fixed:] = self.trainable_weight\n",
    "        return nn.functional.embedding(inp, self.weight, None, None, 2.0, False, False)\n",
    "\n",
    "    \n",
    "class Vocabulary(object):\n",
    "    START_TOKEN = '<start>'\n",
    "    END_TOKEN = '<end>'\n",
    "    PAD_TOKEN = '<pad>'\n",
    "    CONTROL_WORDS = [START_TOKEN, END_TOKEN, PAD_TOKEN]\n",
    "    \n",
    "    def __init__(self, non_control_words):\n",
    "        self.control_words_count = len(CaptionGenerator.CONTROL_WORDS)\n",
    "        \n",
    "        self.vocab = [w for w in non_control_words] + CaptionGenerator.CONTROL_WORDS\n",
    "        self.non_control_words = self.vocab[:-self.control_words_count]\n",
    "        self.word2idx = {w: i for (i, w) in enumerate(self.vocab)}\n",
    "        \n",
    "    def get_idx(self, word):\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def get_start_token_idx(self):\n",
    "        return self.word2idx[Vocabulary.START_TOKEN]\n",
    "    \n",
    "    def get_end_token_idx(self):\n",
    "        return self.word2idx[Vocabulary.END_TOKEN]\n",
    "    \n",
    "    def get_pad_token_idx(self):\n",
    "        return self.word2idx[Vocabulary.PAD_TOKEN]\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    \n",
    "class CaptionGenerator(nn.Module):\n",
    "    CONTROL_WORDS = ['<start>', '<end>', '<pad>']\n",
    "    \n",
    "    def __init__(self, vocab, word2vec, encoded_image_size, hidden_size=512, rnn_layers_num=1):\n",
    "        \"\"\"\n",
    "        :param vocab: set of all the words (as strings) in the vocabulary other than CONTROL_WORDS\n",
    "        :param word2vec: one of the pre-trained models from torchnlp library\n",
    "        \"\"\"\n",
    "        super(CaptionGenerator, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.embed = PartiallyFixedEmbedding(word2vec[vocab.non_control_words], vocab.control_words_count)\n",
    "        \n",
    "        self.initial_hidden_state = nn.Linear(encoded_image_size, hidden_size)\n",
    "        self.initial_cell_state = nn.Linear(encoded_image_size, hidden_size)\n",
    "        \n",
    "        self.recurrent_unit = nn.LSTM(word2vec.dim, hidden_size, rnn_layers_num, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, vocab.size())\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        embeddings = self.embed(captions)\n",
    "        inputs_packed = pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "        \n",
    "        initial_hidden = self.initial_hidden_state(features)\n",
    "        initial_hidden = initial_hidden.view(-1, initial_hidden.shape[0], initial_hidden.shape[1])\n",
    "        initial_cell = self.initial_cell_state(features)\n",
    "        initial_cell = initial_cell.view(-1, initial_cell.shape[0], initial_cell.shape[1])\n",
    "        \n",
    "        hiddens, _ = self.recurrent_unit(inputs_packed, (initial_hidden, initial_cell))\n",
    "        hiddens, lengths = pad_packed_sequence(hiddens, batch_first=True)\n",
    "        outputs = self.linear(hiddens) # maybe a softmax after that?\n",
    "        return outputs, lengths\n",
    "    \n",
    "    def to_sentence(self, forward_out, lengths):\n",
    "        _, tops = forward_out.topk(1)\n",
    "        tops = tops.view(tops.shape[0], tops.shape[1])\n",
    "        sentences = []\n",
    "        for sentence in tops:\n",
    "            words = []\n",
    "            for idx in sentence:\n",
    "                words.append(self.vocab.vocab[idx])\n",
    "            sentences.append(' '.join(words))\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get some data. We downloaded and unpacked the Flickr8K dataset from http://academictorrents.com/details/9dea07ba660a722ae1008c4c8afdd303b6f6e53b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class Flickr8KDataset(data.Dataset):\n",
    "    def __init__(self, tokens_path, images_path):\n",
    "        self.img_names_and_captions = []\n",
    "        self.images_path = images_path\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)), # TODO: maybe add some cropping or sth? this will squash most of the images\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # normalization required by resnet\n",
    "        ])\n",
    "        with open(tokens_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                img_name = parts[0].split('#')[0] # each image has a number of captions - but we don't care how many\n",
    "                tokens = parts[1:(-1 if parts[-1] == '.' else len(parts))] # tokens are everything apart from the image name and the final period\n",
    "                \n",
    "                self.img_names_and_captions.append((img_name, list(map(lambda t: t.lower(), tokens))))\n",
    "                \n",
    "        self.all_tokens = set()\n",
    "        for _, tokens in self.img_names_and_captions:\n",
    "            for token in tokens:\n",
    "                self.all_tokens.add(token)\n",
    "                \n",
    "        self.vocab = Vocabulary(self.all_tokens)\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.img_names_and_captions[idx]\n",
    "        img_path = path.join(self.images_path, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        caption = torch.tensor([self.vocab.get_start_token_idx()] + \n",
    "                               [self.vocab.get_idx(token) for token in caption] + \n",
    "                               [self.vocab.get_end_token_idx()])\n",
    "        return image, caption\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names_and_captions)\n",
    "    \n",
    "    \n",
    "def get_flickr8k_dataloader(tokens_path, images_path, batch_size=32, shuffle=True, num_workers=2):\n",
    "    flickr = Flickr8KDataset(tokens_path, images_path)\n",
    "    \n",
    "    def make_batch(data):\n",
    "        # sort data by caption length\n",
    "        data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "        images, captions = zip(*data)\n",
    "\n",
    "        # Merge image tensors (stack)\n",
    "        images = torch.stack(images, 0)\n",
    "\n",
    "        # Merge captions\n",
    "        caption_lengths = [len(caption) for caption in captions]\n",
    "\n",
    "        # zero-matrix num_captions x caption_max_length\n",
    "        padded_captions = torch.empty(len(captions), max(caption_lengths)).fill_(flickr.vocab.get_pad_token_idx()).long()\n",
    "\n",
    "        # fill the zero-matrix with captions. the remaining zeros are padding\n",
    "        for i, caption in enumerate(captions):\n",
    "            end = caption_lengths[i]\n",
    "            padded_captions[i, :end] = caption[:end]\n",
    "        return images, padded_captions, caption_lengths\n",
    "    \n",
    "    return data.DataLoader(dataset=flickr,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=shuffle,\n",
    "                           num_workers=num_workers,\n",
    "                           collate_fn=make_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shapes of the data in one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images:  \ttorch.Size([32, 3, 224, 224])\n",
      "captions:\ttorch.Size([32, 17])\n",
      "lengths: \t[17, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 13, 13, 13, 13, 12, 12, 11, 11, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "dl = get_flickr8k_dataloader('../../Flickr8k/Flickr8k_text/Flickr8k.token.txt', '../../Flickr8k/Flickr8k_Dataset/Flicker8k_Dataset')\n",
    "images, captions, lengths = next(iter(dl))\n",
    "print(f\"images:  \\t{images.shape}\")\n",
    "print(f\"captions:\\t{captions.shape}\")\n",
    "print(f\"lengths: \\t{lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded images: torch.Size([32, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6978,  0.6970,  1.1099,  ..., -0.2639, -0.2276,  0.4812],\n",
       "        [-0.2985,  0.1557,  0.0748,  ..., -0.5385, -0.6335,  1.0926],\n",
       "        [-0.1242,  0.5914,  0.8516,  ..., -1.0254, -0.4933,  0.7090],\n",
       "        ...,\n",
       "        [ 0.4618,  0.2589,  0.9273,  ..., -0.8678, -0.3088,  0.6960],\n",
       "        [-1.0010,  0.3153, -0.0195,  ..., -0.8871, -1.1349,  1.2005],\n",
       "        [-0.4832,  0.8630,  0.8718,  ..., -0.1795,  0.0482, -0.3214]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_encoding_length = 1000\n",
    "\n",
    "ie = ImageEncoder(image_encoding_length)\n",
    "features = ie.forward(images)\n",
    "print(f\"encoded images: {features.shape}\")\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated: torch.Size([32, 17, 8921])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0106,  0.0343, -0.1044,  ..., -0.1066,  0.0839,  0.0504],\n",
       "         [ 0.0219,  0.0362, -0.1248,  ..., -0.1207,  0.0773, -0.0157],\n",
       "         [ 0.0103,  0.0444, -0.0900,  ..., -0.0469,  0.0734, -0.0253],\n",
       "         ...,\n",
       "         [ 0.0033,  0.0377,  0.0108,  ..., -0.0174,  0.0425, -0.0759],\n",
       "         [-0.0052,  0.0188, -0.0224,  ..., -0.0298,  0.0365, -0.0826],\n",
       "         [-0.0112,  0.0248, -0.0102,  ..., -0.0457,  0.0313, -0.0658]],\n",
       "\n",
       "        [[-0.0206,  0.1056, -0.1236,  ..., -0.0462,  0.0688,  0.0421],\n",
       "         [-0.0129,  0.0684, -0.0445,  ..., -0.0419,  0.0564,  0.0019],\n",
       "         [ 0.0217,  0.0606, -0.0295,  ..., -0.0258,  0.0775, -0.0440],\n",
       "         ...,\n",
       "         [-0.0075,  0.0473,  0.0005,  ..., -0.0598,  0.0523, -0.0395],\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291],\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291]],\n",
       "\n",
       "        [[ 0.0201,  0.0334, -0.0108,  ..., -0.0726,  0.0386,  0.0229],\n",
       "         [ 0.0093,  0.0498, -0.0047,  ..., -0.0337,  0.0402, -0.0348],\n",
       "         [ 0.0380,  0.0343,  0.0019,  ..., -0.0471,  0.0302, -0.0921],\n",
       "         ...,\n",
       "         [-0.0283,  0.0562,  0.0189,  ..., -0.0259,  0.0224, -0.0589],\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291],\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0162,  0.1012, -0.1162,  ..., -0.0495,  0.1336,  0.0240],\n",
       "         [-0.0004,  0.0687, -0.0400,  ..., -0.0564,  0.0969, -0.0024],\n",
       "         [ 0.0302,  0.0482, -0.0347,  ..., -0.0684,  0.0735, -0.0310],\n",
       "         ...,\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291],\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291],\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291]],\n",
       "\n",
       "        [[ 0.0631,  0.0865, -0.0451,  ..., -0.2100,  0.0603,  0.0802],\n",
       "         [ 0.0415,  0.0729, -0.0057,  ..., -0.1131,  0.0546,  0.0134],\n",
       "         [ 0.0419,  0.0928,  0.0039,  ..., -0.0727,  0.0650, -0.0026],\n",
       "         ...,\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291],\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291],\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291]],\n",
       "\n",
       "        [[ 0.0197,  0.0534, -0.0419,  ..., -0.0850,  0.1076,  0.0079],\n",
       "         [ 0.0044,  0.0493, -0.0102,  ..., -0.0599,  0.0796, -0.0056],\n",
       "         [ 0.0324,  0.0316,  0.0008,  ..., -0.0606,  0.0499, -0.0718],\n",
       "         ...,\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291],\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291],\n",
       "         [-0.0097,  0.0249,  0.0298,  ..., -0.0380,  0.0341, -0.0291]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchnlp.word_to_vector import GloVe\n",
    "\n",
    "vocab = dl.dataset.vocab\n",
    "cg = CaptionGenerator(vocab, GloVe(), image_encoding_length)\n",
    "\n",
    "generated, lengths = cg.forward(features, captions, lengths)\n",
    "print(f\"generated: {generated.shape}\")\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bath sequined appears wrestle scubba scubba giong steps fanning course giong scubba driver-side driver-side reaching mule garter',\n",
       " 'bath bath tophats social shoeless retreiver clown ejected limo limo limo stride arizona ticket garter checkered checkered',\n",
       " 'challenging romp gettnig reaching canned mule sewn glancing driver-side billiards cavort appears ticket slices slices checkered checkered',\n",
       " 'scoring watched jacked driver-side move scubba bow rocky rocky scubba scubba scubba scubba prances prances checkered checkered',\n",
       " 'aig driver-side driver-side slices mess car driver-side driver-side car withdrawing woods giong social kid-sized prances checkered checkered',\n",
       " 'aig mosaic car rocky responders earpiece stride rocky slices driver-side innertubes driver-side rocky rocky rocky checkered checkered',\n",
       " 'pit mosaic dave watched giong driver-side driver-side driver-side mule giong giong scubba ticket withdrawing withdrawing checkered checkered',\n",
       " 'challenging paraglide gettnig canned driver-side thong rocky scubba backing giong giong giong together has checkered checkered checkered',\n",
       " 'attaching watched social shoeless giong scubba ticket withdrawing driver-side driver-side driver-side driver-side giong glancing checkered checkered checkered',\n",
       " 'gesturing kerry gettnig rocky scubba biek scubba jersy plays reaching driver-side earpiece scubba scubba checkered checkered checkered',\n",
       " 'romp romp car ticket tw tw together slices scubba scubba scubba mule cavort limo checkered checkered checkered',\n",
       " 'bath gettnig drap drap obscene clown driver-side index driver-side scubba driver-side some scubba checkered checkered checkered checkered',\n",
       " 'romp mosaic experiences talking scubba scubba rocky talking rocky noy giong seperated prances checkered checkered checkered checkered',\n",
       " 'technical technical creek rocky driver-side giong rocky giong giong scubba ticket formed rocky checkered checkered checkered checkered',\n",
       " 'bended watched watched arizona giong scubba scubba stride giong stride index glancing rocky checkered checkered checkered checkered',\n",
       " 'retangular watched experiences talking rocky rocky scubba driver-side talking noy rocky rocky checkered checkered checkered checkered checkered',\n",
       " 'posts trio shoeless giong rocky ticket canned rocky rocky cries scubba rocky checkered checkered checkered checkered checkered',\n",
       " 'watched bridesmaids miniskirts sword their hoolahoops ticket mule scubba popper some checkered checkered checkered checkered checkered checkered',\n",
       " 'even rabbits drap rocky scubba rocky scubba scubba driver-side driver-side belly-surfing checkered checkered checkered checkered checkered checkered',\n",
       " 'romp shadow-dappled shadow-dappled shadow-dappled noy scubba mule rocky circus rocky checkered checkered checkered checkered checkered checkered checkered',\n",
       " 'bounces watched vert rocky shadow-dappled scubba scubba struck prances prances checkered checkered checkered checkered checkered checkered checkered',\n",
       " 'appears compound choice canned biek obscene rocky driver-side driver-side prances checkered checkered checkered checkered checkered checkered checkered',\n",
       " 'beaks ballons drap multiracial ticket giong driver-side driver-side scubba glancing checkered checkered checkered checkered checkered checkered checkered',\n",
       " 'bath glancing return rabbits casting attrative ticket kilt exercising rocky checkered checkered checkered checkered checkered checkered checkered',\n",
       " 'headlamp mule scubba rocky scaling scaling shoeless shoeless shoeless checkered checkered checkered checkered checkered checkered checkered checkered',\n",
       " 'soup illustration talking talking giong giong social social social checkered checkered checkered checkered checkered checkered checkered checkered',\n",
       " 'blades mosaic giong fellow greenish brake driver-side mule garter checkered checkered checkered checkered checkered checkered checkered checkered',\n",
       " 'parka watched scubba watched passifier scubba scubba scubba scubba checkered checkered checkered checkered checkered checkered checkered checkered',\n",
       " 'bath magic jockey exercising exercising scubba trot trot trot checkered checkered checkered checkered checkered checkered checkered checkered',\n",
       " 'heading watched drap fellow fellow tw awe rocky checkered checkered checkered checkered checkered checkered checkered checkered checkered',\n",
       " 'gentle either atop comfort rocky driver-side halves rocky checkered checkered checkered checkered checkered checkered checkered checkered checkered',\n",
       " 'goofy watched away canned mule itself community film checkered checkered checkered checkered checkered checkered checkered checkered checkered']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg.to_sentence(generated, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed a project of Machine Learning with spark ML, in this assignment, we will be swithing to the context of Deep Learning with Tensorflow/Keras by two tasks:\n",
    "- Task1: Image Classification with CNN\n",
    "- Task2: Image captioning with a combination of CNN and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Going Deeper with convolutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before **Inception v1** (**GoogLeNet**), which is the winner of the **ILSVRC** (ImageNet Large Scale Visual Recognition Competition) in 2014, most popular CNNs just stacked convolution layers deeper and deeper, hoping to get better performance.\n",
    "The Inception network, however, uses a lot of tricks to improve performance in terms of speed and accuracy.\n",
    "Compared to other networks, **Inception v1** has significant improvement over **ZFNet** (the winner in 2013) and **AlexNet** (the winner in 2012), and has relatively lower error rate compared with the VGGNet.\n",
    "\n",
    "In this task, we will be implementing the inception architecture [in this paper](https://arxiv.org/abs/1409.4842) with TensorFlow/Keras/Pytorch. \n",
    "\n",
    "The goal of this task is to understand how to write code to build the model, as long as you can verify the correctness of the code (e.g., through Keras model summary), it is not necessary to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/mrobakowski/.conda/lib/python3.7/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy in /home/mrobakowski/.conda/lib/python3.7/site-packages (from torch) (1.17.4)\n",
      "Requirement already satisfied: torchsummary in /home/mrobakowski/.conda/lib/python3.7/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "# conda env torch_planet\n",
    "!pip install torch\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspired from https://github.com/pytorch/vision/blob/master/torchvision/models/googlenet.py\n",
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.jit.annotations import Optional, Tuple\n",
    "from torch import Tensor\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GoogLeNetOutputs = namedtuple('GoogLeNetOutputs', ['logits', 'aux_logits2', 'aux_logits1'])\n",
    "GoogLeNetOutputs.__annotations__ = {'logits': Tensor, 'aux_logits2': Optional[Tensor],\n",
    "                                    'aux_logits1': Optional[Tensor]}\n",
    "\n",
    "# _GoogLeNetOutputs set here for backwards compat\n",
    "_GoogLeNetOutputs = GoogLeNetOutputs\n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000, aux_logits=True, blocks=None):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        if blocks is None:\n",
    "            blocks = [BasicConv2d, Inception, InceptionAux]\n",
    "        assert len(blocks) == 3\n",
    "        conv_block = blocks[0]\n",
    "        inception_block = blocks[1]\n",
    "        inception_aux_block = blocks[2]\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "        self.conv2 = conv_block(64, 64, kernel_size=1)\n",
    "        self.conv3 = conv_block(64, 192, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = inception_block(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = inception_block(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception4a = inception_block(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = inception_block(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = inception_block(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = inception_block(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = inception_block(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception5a = inception_block(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = inception_block(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        if aux_logits:\n",
    "            self.aux1 = inception_aux_block(512, num_classes)\n",
    "            self.aux2 = inception_aux_block(528, num_classes)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                X = stats.truncnorm(-2, 2, scale=0.01)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def _forward(self, x):\n",
    "        # type: (Tensor) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]\n",
    "        # N x 3 x 224 x 224\n",
    "        x = self.conv1(x)\n",
    "        # N x 64 x 112 x 112\n",
    "        x = self.maxpool1(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv2(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv3(x)\n",
    "        # N x 192 x 56 x 56\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # N x 192 x 28 x 28\n",
    "        x = self.inception3a(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        x = self.inception3b(x)\n",
    "        # N x 480 x 28 x 28\n",
    "        x = self.maxpool3(x)\n",
    "        # N x 480 x 14 x 14\n",
    "        x = self.inception4a(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if aux_defined:\n",
    "            aux1 = self.aux1(x)\n",
    "        else:\n",
    "            aux1 = None\n",
    "\n",
    "        x = self.inception4b(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4c(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4d(x)\n",
    "        # N x 528 x 14 x 14\n",
    "        if aux_defined:\n",
    "            aux2 = self.aux2(x)\n",
    "        else:\n",
    "            aux2 = None\n",
    "\n",
    "        x = self.inception4e(x)\n",
    "        # N x 832 x 14 x 14\n",
    "        x = self.maxpool4(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5a(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5b(x)\n",
    "        # N x 1024 x 7 x 7\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # N x 1024 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        return x, aux2, aux1\n",
    "\n",
    "#     @torch.jit.unused\n",
    "    def eager_outputs(self, x, aux2, aux1):\n",
    "        # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> GoogLeNetOutputs\n",
    "        if self.training and self.aux_logits:\n",
    "            return _GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # type: (Tensor) -> GoogLeNetOutputs\n",
    "        x, aux1, aux2 = self._forward(x)\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if torch.jit.is_scripting():\n",
    "            if not aux_defined:\n",
    "                warnings.warn(\"Scripted GoogleNet always returns GoogleNetOutputs Tuple\")\n",
    "            return GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return self.eager_outputs(x, aux2, aux1)\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    __constants__ = ['branch2', 'branch3', 'branch4']\n",
    "\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj,\n",
    "                 conv_block=None):\n",
    "        super(Inception, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            conv_block(in_channels, ch3x3red, kernel_size=1),\n",
    "            conv_block(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            conv_block(in_channels, ch5x5red, kernel_size=1),\n",
    "            conv_block(ch5x5red, ch5x5, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n",
    "            conv_block(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        outputs = [branch1, branch2, branch3, branch4]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, conv_block=None):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        # N x 1024\n",
    "        x = F.dropout(x, 0.7, training=self.training)\n",
    "        # N x 1024\n",
    "        x = self.fc2(x)\n",
    "        # N x 1000 (num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= GoogLeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "       BasicConv2d-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "       BasicConv2d-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8          [-1, 192, 56, 56]         110,592\n",
      "       BatchNorm2d-9          [-1, 192, 56, 56]             384\n",
      "      BasicConv2d-10          [-1, 192, 56, 56]               0\n",
      "        MaxPool2d-11          [-1, 192, 28, 28]               0\n",
      "           Conv2d-12           [-1, 64, 28, 28]          12,288\n",
      "      BatchNorm2d-13           [-1, 64, 28, 28]             128\n",
      "      BasicConv2d-14           [-1, 64, 28, 28]               0\n",
      "           Conv2d-15           [-1, 96, 28, 28]          18,432\n",
      "      BatchNorm2d-16           [-1, 96, 28, 28]             192\n",
      "      BasicConv2d-17           [-1, 96, 28, 28]               0\n",
      "           Conv2d-18          [-1, 128, 28, 28]         110,592\n",
      "      BatchNorm2d-19          [-1, 128, 28, 28]             256\n",
      "      BasicConv2d-20          [-1, 128, 28, 28]               0\n",
      "           Conv2d-21           [-1, 16, 28, 28]           3,072\n",
      "      BatchNorm2d-22           [-1, 16, 28, 28]              32\n",
      "      BasicConv2d-23           [-1, 16, 28, 28]               0\n",
      "           Conv2d-24           [-1, 32, 28, 28]           4,608\n",
      "      BatchNorm2d-25           [-1, 32, 28, 28]              64\n",
      "      BasicConv2d-26           [-1, 32, 28, 28]               0\n",
      "        MaxPool2d-27          [-1, 192, 28, 28]               0\n",
      "           Conv2d-28           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-29           [-1, 32, 28, 28]              64\n",
      "      BasicConv2d-30           [-1, 32, 28, 28]               0\n",
      "        Inception-31          [-1, 256, 28, 28]               0\n",
      "           Conv2d-32          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-33          [-1, 128, 28, 28]             256\n",
      "      BasicConv2d-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "      BasicConv2d-37          [-1, 128, 28, 28]               0\n",
      "           Conv2d-38          [-1, 192, 28, 28]         221,184\n",
      "      BatchNorm2d-39          [-1, 192, 28, 28]             384\n",
      "      BasicConv2d-40          [-1, 192, 28, 28]               0\n",
      "           Conv2d-41           [-1, 32, 28, 28]           8,192\n",
      "      BatchNorm2d-42           [-1, 32, 28, 28]              64\n",
      "      BasicConv2d-43           [-1, 32, 28, 28]               0\n",
      "           Conv2d-44           [-1, 96, 28, 28]          27,648\n",
      "      BatchNorm2d-45           [-1, 96, 28, 28]             192\n",
      "      BasicConv2d-46           [-1, 96, 28, 28]               0\n",
      "        MaxPool2d-47          [-1, 256, 28, 28]               0\n",
      "           Conv2d-48           [-1, 64, 28, 28]          16,384\n",
      "      BatchNorm2d-49           [-1, 64, 28, 28]             128\n",
      "      BasicConv2d-50           [-1, 64, 28, 28]               0\n",
      "        Inception-51          [-1, 480, 28, 28]               0\n",
      "        MaxPool2d-52          [-1, 480, 14, 14]               0\n",
      "           Conv2d-53          [-1, 192, 14, 14]          92,160\n",
      "      BatchNorm2d-54          [-1, 192, 14, 14]             384\n",
      "      BasicConv2d-55          [-1, 192, 14, 14]               0\n",
      "           Conv2d-56           [-1, 96, 14, 14]          46,080\n",
      "      BatchNorm2d-57           [-1, 96, 14, 14]             192\n",
      "      BasicConv2d-58           [-1, 96, 14, 14]               0\n",
      "           Conv2d-59          [-1, 208, 14, 14]         179,712\n",
      "      BatchNorm2d-60          [-1, 208, 14, 14]             416\n",
      "      BasicConv2d-61          [-1, 208, 14, 14]               0\n",
      "           Conv2d-62           [-1, 16, 14, 14]           7,680\n",
      "      BatchNorm2d-63           [-1, 16, 14, 14]              32\n",
      "      BasicConv2d-64           [-1, 16, 14, 14]               0\n",
      "           Conv2d-65           [-1, 48, 14, 14]           6,912\n",
      "      BatchNorm2d-66           [-1, 48, 14, 14]              96\n",
      "      BasicConv2d-67           [-1, 48, 14, 14]               0\n",
      "        MaxPool2d-68          [-1, 480, 14, 14]               0\n",
      "           Conv2d-69           [-1, 64, 14, 14]          30,720\n",
      "      BatchNorm2d-70           [-1, 64, 14, 14]             128\n",
      "      BasicConv2d-71           [-1, 64, 14, 14]               0\n",
      "        Inception-72          [-1, 512, 14, 14]               0\n",
      "           Conv2d-73            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-74            [-1, 128, 4, 4]             256\n",
      "      BasicConv2d-75            [-1, 128, 4, 4]               0\n",
      "           Linear-76                 [-1, 1024]       2,098,176\n",
      "           Linear-77                 [-1, 1000]       1,025,000\n",
      "     InceptionAux-78                 [-1, 1000]               0\n",
      "           Conv2d-79          [-1, 160, 14, 14]          81,920\n",
      "      BatchNorm2d-80          [-1, 160, 14, 14]             320\n",
      "      BasicConv2d-81          [-1, 160, 14, 14]               0\n",
      "           Conv2d-82          [-1, 112, 14, 14]          57,344\n",
      "      BatchNorm2d-83          [-1, 112, 14, 14]             224\n",
      "      BasicConv2d-84          [-1, 112, 14, 14]               0\n",
      "           Conv2d-85          [-1, 224, 14, 14]         225,792\n",
      "      BatchNorm2d-86          [-1, 224, 14, 14]             448\n",
      "      BasicConv2d-87          [-1, 224, 14, 14]               0\n",
      "           Conv2d-88           [-1, 24, 14, 14]          12,288\n",
      "      BatchNorm2d-89           [-1, 24, 14, 14]              48\n",
      "      BasicConv2d-90           [-1, 24, 14, 14]               0\n",
      "           Conv2d-91           [-1, 64, 14, 14]          13,824\n",
      "      BatchNorm2d-92           [-1, 64, 14, 14]             128\n",
      "      BasicConv2d-93           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-94          [-1, 512, 14, 14]               0\n",
      "           Conv2d-95           [-1, 64, 14, 14]          32,768\n",
      "      BatchNorm2d-96           [-1, 64, 14, 14]             128\n",
      "      BasicConv2d-97           [-1, 64, 14, 14]               0\n",
      "        Inception-98          [-1, 512, 14, 14]               0\n",
      "           Conv2d-99          [-1, 128, 14, 14]          65,536\n",
      "     BatchNorm2d-100          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-101          [-1, 128, 14, 14]               0\n",
      "          Conv2d-102          [-1, 128, 14, 14]          65,536\n",
      "     BatchNorm2d-103          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-104          [-1, 128, 14, 14]               0\n",
      "          Conv2d-105          [-1, 256, 14, 14]         294,912\n",
      "     BatchNorm2d-106          [-1, 256, 14, 14]             512\n",
      "     BasicConv2d-107          [-1, 256, 14, 14]               0\n",
      "          Conv2d-108           [-1, 24, 14, 14]          12,288\n",
      "     BatchNorm2d-109           [-1, 24, 14, 14]              48\n",
      "     BasicConv2d-110           [-1, 24, 14, 14]               0\n",
      "          Conv2d-111           [-1, 64, 14, 14]          13,824\n",
      "     BatchNorm2d-112           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-113           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-114          [-1, 512, 14, 14]               0\n",
      "          Conv2d-115           [-1, 64, 14, 14]          32,768\n",
      "     BatchNorm2d-116           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-117           [-1, 64, 14, 14]               0\n",
      "       Inception-118          [-1, 512, 14, 14]               0\n",
      "          Conv2d-119          [-1, 112, 14, 14]          57,344\n",
      "     BatchNorm2d-120          [-1, 112, 14, 14]             224\n",
      "     BasicConv2d-121          [-1, 112, 14, 14]               0\n",
      "          Conv2d-122          [-1, 144, 14, 14]          73,728\n",
      "     BatchNorm2d-123          [-1, 144, 14, 14]             288\n",
      "     BasicConv2d-124          [-1, 144, 14, 14]               0\n",
      "          Conv2d-125          [-1, 288, 14, 14]         373,248\n",
      "     BatchNorm2d-126          [-1, 288, 14, 14]             576\n",
      "     BasicConv2d-127          [-1, 288, 14, 14]               0\n",
      "          Conv2d-128           [-1, 32, 14, 14]          16,384\n",
      "     BatchNorm2d-129           [-1, 32, 14, 14]              64\n",
      "     BasicConv2d-130           [-1, 32, 14, 14]               0\n",
      "          Conv2d-131           [-1, 64, 14, 14]          18,432\n",
      "     BatchNorm2d-132           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-133           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-134          [-1, 512, 14, 14]               0\n",
      "          Conv2d-135           [-1, 64, 14, 14]          32,768\n",
      "     BatchNorm2d-136           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-137           [-1, 64, 14, 14]               0\n",
      "       Inception-138          [-1, 528, 14, 14]               0\n",
      "          Conv2d-139            [-1, 128, 4, 4]          67,584\n",
      "     BatchNorm2d-140            [-1, 128, 4, 4]             256\n",
      "     BasicConv2d-141            [-1, 128, 4, 4]               0\n",
      "          Linear-142                 [-1, 1024]       2,098,176\n",
      "          Linear-143                 [-1, 1000]       1,025,000\n",
      "    InceptionAux-144                 [-1, 1000]               0\n",
      "          Conv2d-145          [-1, 256, 14, 14]         135,168\n",
      "     BatchNorm2d-146          [-1, 256, 14, 14]             512\n",
      "     BasicConv2d-147          [-1, 256, 14, 14]               0\n",
      "          Conv2d-148          [-1, 160, 14, 14]          84,480\n",
      "     BatchNorm2d-149          [-1, 160, 14, 14]             320\n",
      "     BasicConv2d-150          [-1, 160, 14, 14]               0\n",
      "          Conv2d-151          [-1, 320, 14, 14]         460,800\n",
      "     BatchNorm2d-152          [-1, 320, 14, 14]             640\n",
      "     BasicConv2d-153          [-1, 320, 14, 14]               0\n",
      "          Conv2d-154           [-1, 32, 14, 14]          16,896\n",
      "     BatchNorm2d-155           [-1, 32, 14, 14]              64\n",
      "     BasicConv2d-156           [-1, 32, 14, 14]               0\n",
      "          Conv2d-157          [-1, 128, 14, 14]          36,864\n",
      "     BatchNorm2d-158          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-159          [-1, 128, 14, 14]               0\n",
      "       MaxPool2d-160          [-1, 528, 14, 14]               0\n",
      "          Conv2d-161          [-1, 128, 14, 14]          67,584\n",
      "     BatchNorm2d-162          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-163          [-1, 128, 14, 14]               0\n",
      "       Inception-164          [-1, 832, 14, 14]               0\n",
      "       MaxPool2d-165            [-1, 832, 7, 7]               0\n",
      "          Conv2d-166            [-1, 256, 7, 7]         212,992\n",
      "     BatchNorm2d-167            [-1, 256, 7, 7]             512\n",
      "     BasicConv2d-168            [-1, 256, 7, 7]               0\n",
      "          Conv2d-169            [-1, 160, 7, 7]         133,120\n",
      "     BatchNorm2d-170            [-1, 160, 7, 7]             320\n",
      "     BasicConv2d-171            [-1, 160, 7, 7]               0\n",
      "          Conv2d-172            [-1, 320, 7, 7]         460,800\n",
      "     BatchNorm2d-173            [-1, 320, 7, 7]             640\n",
      "     BasicConv2d-174            [-1, 320, 7, 7]               0\n",
      "          Conv2d-175             [-1, 32, 7, 7]          26,624\n",
      "     BatchNorm2d-176             [-1, 32, 7, 7]              64\n",
      "     BasicConv2d-177             [-1, 32, 7, 7]               0\n",
      "          Conv2d-178            [-1, 128, 7, 7]          36,864\n",
      "     BatchNorm2d-179            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-180            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-181            [-1, 832, 7, 7]               0\n",
      "          Conv2d-182            [-1, 128, 7, 7]         106,496\n",
      "     BatchNorm2d-183            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-184            [-1, 128, 7, 7]               0\n",
      "       Inception-185            [-1, 832, 7, 7]               0\n",
      "          Conv2d-186            [-1, 384, 7, 7]         319,488\n",
      "     BatchNorm2d-187            [-1, 384, 7, 7]             768\n",
      "     BasicConv2d-188            [-1, 384, 7, 7]               0\n",
      "          Conv2d-189            [-1, 192, 7, 7]         159,744\n",
      "     BatchNorm2d-190            [-1, 192, 7, 7]             384\n",
      "     BasicConv2d-191            [-1, 192, 7, 7]               0\n",
      "          Conv2d-192            [-1, 384, 7, 7]         663,552\n",
      "     BatchNorm2d-193            [-1, 384, 7, 7]             768\n",
      "     BasicConv2d-194            [-1, 384, 7, 7]               0\n",
      "          Conv2d-195             [-1, 48, 7, 7]          39,936\n",
      "     BatchNorm2d-196             [-1, 48, 7, 7]              96\n",
      "     BasicConv2d-197             [-1, 48, 7, 7]               0\n",
      "          Conv2d-198            [-1, 128, 7, 7]          55,296\n",
      "     BatchNorm2d-199            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-200            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-201            [-1, 832, 7, 7]               0\n",
      "          Conv2d-202            [-1, 128, 7, 7]         106,496\n",
      "     BatchNorm2d-203            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-204            [-1, 128, 7, 7]               0\n",
      "       Inception-205           [-1, 1024, 7, 7]               0\n",
      "AdaptiveAvgPool2d-206           [-1, 1024, 1, 1]               0\n",
      "         Dropout-207                 [-1, 1024]               0\n",
      "          Linear-208                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 13,004,888\n",
      "Trainable params: 13,004,888\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 94.25\n",
      "Params size (MB): 49.61\n",
      "Estimated Total Size (MB): 144.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Show and Tell: A Neural Image Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically describing the content of an image is a fundamental problem in AI that connects *computer vision* and *natural language processing*.\n",
    "In this task, we will be looking into how we can use CNNs and RNNs to build an Image Caption Generator.\n",
    "\n",
    "Specifically, you will be implementing and training the model [in this paper](https://arxiv.org/abs/1411.4555) with TensorFlow/Keras on one of the datasets mentioned in the paper.\n",
    "\n",
    "To lighten the burden on training the network, you can use any pretrained network in [tf.keras.applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoder\n",
    "First we fetch a pre-trained convolutional neural network and tweak it slightly so it's not focused on classification anymore. We'll use this to encode our image and then feed that to the caption generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Network to encode an image\"\"\"\n",
    "    def __init__(self, out_dim=1000):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.resnet = models.resnet152(pretrained=True)\n",
    "        # it's pretrained, so let's make it not change\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        # the last layer of resnet is Linear that is used for classification\n",
    "        # we'll change its size, mark it as learnable, and initialize it randomly, since it's not going to be classification anymore\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, out_dim) # constructing it marks it as learnable as well\n",
    "        assert(all(p.requires_grad for p in self.resnet.fc.parameters()))\n",
    "        self.resnet.fc.weight.data.normal_(0, 0.02) # weights to a small number\n",
    "        self.resnet.fc.bias.data.fill_(0) # bias to zero\n",
    "        \n",
    "        # TODO: maybe it also makes sense to tweak the one layer before that, that is, the pooling layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Generator\n",
    "This is a recurrent neural network that is first initialized with the encoded image and then starts generating words that will be used as the generated caption. The word embeddings are pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartiallyFixedEmbedding(nn.Module): # from https://discuss.pytorch.org/t/updating-part-of-an-embedding-matrix-only-for-out-of-vocab-words/33297/4\n",
    "    \"\"\"\n",
    "    This embedding has an embedding matrix that is split into two parts: fixed and variable.\n",
    "    The fixed part is not changed in the learning process, making it ideal for pretrained vocabularies with a few extra words that we want to learn.\n",
    "    \"\"\"\n",
    "    def __init__(self, fixed_weights, num_to_learn):\n",
    "        super().__init__()\n",
    "        self.num_fixed = fixed_weights.size(0)\n",
    "        self.num_to_learn = num_to_learn\n",
    "        weight = torch.empty(self.num_fixed + num_to_learn, fixed_weights.size(1))\n",
    "        weight[:self.num_fixed] = fixed_weights\n",
    "        self.trainable_weight = nn.Parameter(torch.empty(num_to_learn, fixed_weights.size(1)))\n",
    "        nn.init.kaiming_uniform_(self.trainable_weight)\n",
    "        weight[self.num_fixed:] = self.trainable_weight\n",
    "        self.register_buffer('weight', weight)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        self.weight.detach_()\n",
    "        self.weight[self.num_fixed:] = self.trainable_weight\n",
    "        return nn.functional.embedding(inp, self.weight, None, None, 2.0, False, False)\n",
    "\n",
    "    \n",
    "class Vocabulary(object):\n",
    "    START_TOKEN = '<start>'\n",
    "    END_TOKEN = '<end>'\n",
    "    PAD_TOKEN = '<pad>'\n",
    "    CONTROL_WORDS = [START_TOKEN, END_TOKEN, PAD_TOKEN]\n",
    "    \n",
    "    def __init__(self, non_control_words):\n",
    "        self.control_words_count = len(CaptionGenerator.CONTROL_WORDS)\n",
    "        \n",
    "        self.vocab = [w for w in non_control_words] + CaptionGenerator.CONTROL_WORDS\n",
    "        self.non_control_words = self.vocab[:-self.control_words_count]\n",
    "        self.word2idx = {w: i for (i, w) in enumerate(self.vocab)}\n",
    "        \n",
    "    def get_idx(self, word):\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def get_start_token_idx(self):\n",
    "        return self.word2idx[Vocabulary.START_TOKEN]\n",
    "    \n",
    "    def get_end_token_idx(self):\n",
    "        return self.word2idx[Vocabulary.END_TOKEN]\n",
    "    \n",
    "    def get_pad_token_idx(self):\n",
    "        return self.word2idx[Vocabulary.PAD_TOKEN]\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    \n",
    "class CaptionGenerator(nn.Module):\n",
    "    CONTROL_WORDS = ['<start>', '<end>', '<pad>']\n",
    "    \n",
    "    def __init__(self, vocab, word2vec, hidden_size=512, rnn_layers_num=1):\n",
    "        \"\"\"\n",
    "        :param vocab: set of all the words (as strings) in the vocabulary other than CONTROL_WORDS\n",
    "        :param word2vec: one of the pre-trained models from torchnlp library\n",
    "        \"\"\"\n",
    "        super(CaptionGenerator, self).__init__()\n",
    "        self.embed = PartiallyFixedEmbedding(word2vec[vocab.non_control_words], vocab.control_words_count)\n",
    "        \n",
    "        self.recurrent_unit = nn.LSTM(word2vec.dim, hidden_size, rnn_layers_num, batch_first=True) # TODO: maybe allow to specify RNN, GRU, LSTM?\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, len(vocab.size()))\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        embeddings = self.embed(captions)\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        inputs_packed = pack_padded_sequence(inputs, lengths, batch_first=True)\n",
    "        \n",
    "        hiddens, _ = self.recurrent_unit(inputs_packed)\n",
    "        outputs = self.linear(hiddens[0]) # torch.cat(tuple(map(lambda h: self.linear(h), hiddens)), 0) ???\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get some data. We downloaded and unpacked the Flickr8K dataset from http://academictorrents.com/details/9dea07ba660a722ae1008c4c8afdd303b6f6e53b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from os import path\n",
    "from PIL import Image\n",
    "\n",
    "class Flickr8KDataset(data.Dataset):\n",
    "    def __init__(self, tokens_path, images_path, transform):\n",
    "        self.img_names_and_captions = []\n",
    "        self.images_path = images_path\n",
    "        self.transform = transform\n",
    "        with open(tokens_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                img_name = parts[0].split('#')[0] # each image has a number of captions - but we don't care how many\n",
    "                tokens = parts[1:(-1 if parts[-1] == '.' else len(parts))] # tokens are everything apart from the image name and the final period\n",
    "                \n",
    "                self.img_names_and_captions.append((img_name, list(map(lambda t: t.lower(), tokens))))\n",
    "                \n",
    "        self.all_tokens = set()\n",
    "        for _, tokens in self.img_names_and_captions:\n",
    "            for token in tokens:\n",
    "                self.all_tokens.add(token)\n",
    "                \n",
    "        self.vocab = Vocabulary(self.all_tokens)\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.img_names_and_captions[idx]\n",
    "        img_path = path.join(self.images_path, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        caption = torch.tensor([self.vocab.get_start_token_idx()] + \n",
    "                               [self.vocab.get_idx(token) for token in caption] + \n",
    "                               [self.vocab.get_end_token_idx()])\n",
    "        return image, caption\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names_and_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=375x500 at 0x7F910C060048>,\n",
       " tensor([8918, 6328, 7208, 2861, 6328, 2673, 5902, 2333, 1285, 7700, 6328, 7399,\n",
       "         6160, 1273, 2861, 6498, 3863, 2478, 8919]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Flickr8KDataset('../../Flickr8k/Flickr8k_text/Flickr8k.token.txt', '../../Flickr8k/Flickr8k_Dataset/Flicker8k_Dataset', None)\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed a project of Machine Learning with spark ML, in this assignment, we will be swithing to the context of Deep Learning with Tensorflow/Keras by two tasks:\n",
    "- Task1: Image Classification with CNN\n",
    "- Task2: Image captioning with a combination of CNN and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Going Deeper with convolutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before **Inception v1** (**GoogLeNet**), which is the winner of the **ILSVRC** (ImageNet Large Scale Visual Recognition Competition) in 2014, most popular CNNs just stacked convolution layers deeper and deeper, hoping to get better performance.\n",
    "The Inception network, however, uses a lot of tricks to improve performance in terms of speed and accuracy.\n",
    "Compared to other networks, **Inception v1** has significant improvement over **ZFNet** (the winner in 2013) and **AlexNet** (the winner in 2012), and has relatively lower error rate compared with the VGGNet.\n",
    "\n",
    "In this task, we will be implementing the inception architecture [in this paper](https://arxiv.org/abs/1409.4842) with TensorFlow/Keras/Pytorch. \n",
    "\n",
    "The goal of this task is to understand how to write code to build the model, as long as you can verify the correctness of the code (e.g., through Keras model summary), it is not necessary to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/mrobakowski/.conda/lib/python3.7/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy in /home/mrobakowski/.conda/lib/python3.7/site-packages (from torch) (1.17.4)\n",
      "Requirement already satisfied: torchsummary in /home/mrobakowski/.conda/lib/python3.7/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "# conda env torch_planet\n",
    "!pip install torch\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspired from https://github.com/pytorch/vision/blob/master/torchvision/models/googlenet.py\n",
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.jit.annotations import Optional, Tuple\n",
    "from torch import Tensor\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GoogLeNetOutputs = namedtuple('GoogLeNetOutputs', ['logits', 'aux_logits2', 'aux_logits1'])\n",
    "GoogLeNetOutputs.__annotations__ = {'logits': Tensor, 'aux_logits2': Optional[Tensor],\n",
    "                                    'aux_logits1': Optional[Tensor]}\n",
    "\n",
    "# _GoogLeNetOutputs set here for backwards compat\n",
    "_GoogLeNetOutputs = GoogLeNetOutputs\n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000, aux_logits=True, blocks=None):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        if blocks is None:\n",
    "            blocks = [BasicConv2d, Inception, InceptionAux]\n",
    "        assert len(blocks) == 3\n",
    "        conv_block = blocks[0]\n",
    "        inception_block = blocks[1]\n",
    "        inception_aux_block = blocks[2]\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "        self.conv2 = conv_block(64, 64, kernel_size=1)\n",
    "        self.conv3 = conv_block(64, 192, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = inception_block(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = inception_block(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception4a = inception_block(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = inception_block(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = inception_block(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = inception_block(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = inception_block(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception5a = inception_block(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = inception_block(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        if aux_logits:\n",
    "            self.aux1 = inception_aux_block(512, num_classes)\n",
    "            self.aux2 = inception_aux_block(528, num_classes)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                X = stats.truncnorm(-2, 2, scale=0.01)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def _forward(self, x):\n",
    "        # type: (Tensor) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]\n",
    "        # N x 3 x 224 x 224\n",
    "        x = self.conv1(x)\n",
    "        # N x 64 x 112 x 112\n",
    "        x = self.maxpool1(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv2(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv3(x)\n",
    "        # N x 192 x 56 x 56\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # N x 192 x 28 x 28\n",
    "        x = self.inception3a(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        x = self.inception3b(x)\n",
    "        # N x 480 x 28 x 28\n",
    "        x = self.maxpool3(x)\n",
    "        # N x 480 x 14 x 14\n",
    "        x = self.inception4a(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if aux_defined:\n",
    "            aux1 = self.aux1(x)\n",
    "        else:\n",
    "            aux1 = None\n",
    "\n",
    "        x = self.inception4b(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4c(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4d(x)\n",
    "        # N x 528 x 14 x 14\n",
    "        if aux_defined:\n",
    "            aux2 = self.aux2(x)\n",
    "        else:\n",
    "            aux2 = None\n",
    "\n",
    "        x = self.inception4e(x)\n",
    "        # N x 832 x 14 x 14\n",
    "        x = self.maxpool4(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5a(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5b(x)\n",
    "        # N x 1024 x 7 x 7\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # N x 1024 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        return x, aux2, aux1\n",
    "\n",
    "#     @torch.jit.unused\n",
    "    def eager_outputs(self, x, aux2, aux1):\n",
    "        # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> GoogLeNetOutputs\n",
    "        if self.training and self.aux_logits:\n",
    "            return _GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # type: (Tensor) -> GoogLeNetOutputs\n",
    "        x, aux1, aux2 = self._forward(x)\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if torch.jit.is_scripting():\n",
    "            if not aux_defined:\n",
    "                warnings.warn(\"Scripted GoogleNet always returns GoogleNetOutputs Tuple\")\n",
    "            return GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return self.eager_outputs(x, aux2, aux1)\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    __constants__ = ['branch2', 'branch3', 'branch4']\n",
    "\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj,\n",
    "                 conv_block=None):\n",
    "        super(Inception, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            conv_block(in_channels, ch3x3red, kernel_size=1),\n",
    "            conv_block(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            conv_block(in_channels, ch5x5red, kernel_size=1),\n",
    "            conv_block(ch5x5red, ch5x5, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n",
    "            conv_block(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        outputs = [branch1, branch2, branch3, branch4]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, conv_block=None):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        # N x 1024\n",
    "        x = F.dropout(x, 0.7, training=self.training)\n",
    "        # N x 1024\n",
    "        x = self.fc2(x)\n",
    "        # N x 1000 (num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= GoogLeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "       BasicConv2d-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "       BasicConv2d-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8          [-1, 192, 56, 56]         110,592\n",
      "       BatchNorm2d-9          [-1, 192, 56, 56]             384\n",
      "      BasicConv2d-10          [-1, 192, 56, 56]               0\n",
      "        MaxPool2d-11          [-1, 192, 28, 28]               0\n",
      "           Conv2d-12           [-1, 64, 28, 28]          12,288\n",
      "      BatchNorm2d-13           [-1, 64, 28, 28]             128\n",
      "      BasicConv2d-14           [-1, 64, 28, 28]               0\n",
      "           Conv2d-15           [-1, 96, 28, 28]          18,432\n",
      "      BatchNorm2d-16           [-1, 96, 28, 28]             192\n",
      "      BasicConv2d-17           [-1, 96, 28, 28]               0\n",
      "           Conv2d-18          [-1, 128, 28, 28]         110,592\n",
      "      BatchNorm2d-19          [-1, 128, 28, 28]             256\n",
      "      BasicConv2d-20          [-1, 128, 28, 28]               0\n",
      "           Conv2d-21           [-1, 16, 28, 28]           3,072\n",
      "      BatchNorm2d-22           [-1, 16, 28, 28]              32\n",
      "      BasicConv2d-23           [-1, 16, 28, 28]               0\n",
      "           Conv2d-24           [-1, 32, 28, 28]           4,608\n",
      "      BatchNorm2d-25           [-1, 32, 28, 28]              64\n",
      "      BasicConv2d-26           [-1, 32, 28, 28]               0\n",
      "        MaxPool2d-27          [-1, 192, 28, 28]               0\n",
      "           Conv2d-28           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-29           [-1, 32, 28, 28]              64\n",
      "      BasicConv2d-30           [-1, 32, 28, 28]               0\n",
      "        Inception-31          [-1, 256, 28, 28]               0\n",
      "           Conv2d-32          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-33          [-1, 128, 28, 28]             256\n",
      "      BasicConv2d-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "      BasicConv2d-37          [-1, 128, 28, 28]               0\n",
      "           Conv2d-38          [-1, 192, 28, 28]         221,184\n",
      "      BatchNorm2d-39          [-1, 192, 28, 28]             384\n",
      "      BasicConv2d-40          [-1, 192, 28, 28]               0\n",
      "           Conv2d-41           [-1, 32, 28, 28]           8,192\n",
      "      BatchNorm2d-42           [-1, 32, 28, 28]              64\n",
      "      BasicConv2d-43           [-1, 32, 28, 28]               0\n",
      "           Conv2d-44           [-1, 96, 28, 28]          27,648\n",
      "      BatchNorm2d-45           [-1, 96, 28, 28]             192\n",
      "      BasicConv2d-46           [-1, 96, 28, 28]               0\n",
      "        MaxPool2d-47          [-1, 256, 28, 28]               0\n",
      "           Conv2d-48           [-1, 64, 28, 28]          16,384\n",
      "      BatchNorm2d-49           [-1, 64, 28, 28]             128\n",
      "      BasicConv2d-50           [-1, 64, 28, 28]               0\n",
      "        Inception-51          [-1, 480, 28, 28]               0\n",
      "        MaxPool2d-52          [-1, 480, 14, 14]               0\n",
      "           Conv2d-53          [-1, 192, 14, 14]          92,160\n",
      "      BatchNorm2d-54          [-1, 192, 14, 14]             384\n",
      "      BasicConv2d-55          [-1, 192, 14, 14]               0\n",
      "           Conv2d-56           [-1, 96, 14, 14]          46,080\n",
      "      BatchNorm2d-57           [-1, 96, 14, 14]             192\n",
      "      BasicConv2d-58           [-1, 96, 14, 14]               0\n",
      "           Conv2d-59          [-1, 208, 14, 14]         179,712\n",
      "      BatchNorm2d-60          [-1, 208, 14, 14]             416\n",
      "      BasicConv2d-61          [-1, 208, 14, 14]               0\n",
      "           Conv2d-62           [-1, 16, 14, 14]           7,680\n",
      "      BatchNorm2d-63           [-1, 16, 14, 14]              32\n",
      "      BasicConv2d-64           [-1, 16, 14, 14]               0\n",
      "           Conv2d-65           [-1, 48, 14, 14]           6,912\n",
      "      BatchNorm2d-66           [-1, 48, 14, 14]              96\n",
      "      BasicConv2d-67           [-1, 48, 14, 14]               0\n",
      "        MaxPool2d-68          [-1, 480, 14, 14]               0\n",
      "           Conv2d-69           [-1, 64, 14, 14]          30,720\n",
      "      BatchNorm2d-70           [-1, 64, 14, 14]             128\n",
      "      BasicConv2d-71           [-1, 64, 14, 14]               0\n",
      "        Inception-72          [-1, 512, 14, 14]               0\n",
      "           Conv2d-73            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-74            [-1, 128, 4, 4]             256\n",
      "      BasicConv2d-75            [-1, 128, 4, 4]               0\n",
      "           Linear-76                 [-1, 1024]       2,098,176\n",
      "           Linear-77                 [-1, 1000]       1,025,000\n",
      "     InceptionAux-78                 [-1, 1000]               0\n",
      "           Conv2d-79          [-1, 160, 14, 14]          81,920\n",
      "      BatchNorm2d-80          [-1, 160, 14, 14]             320\n",
      "      BasicConv2d-81          [-1, 160, 14, 14]               0\n",
      "           Conv2d-82          [-1, 112, 14, 14]          57,344\n",
      "      BatchNorm2d-83          [-1, 112, 14, 14]             224\n",
      "      BasicConv2d-84          [-1, 112, 14, 14]               0\n",
      "           Conv2d-85          [-1, 224, 14, 14]         225,792\n",
      "      BatchNorm2d-86          [-1, 224, 14, 14]             448\n",
      "      BasicConv2d-87          [-1, 224, 14, 14]               0\n",
      "           Conv2d-88           [-1, 24, 14, 14]          12,288\n",
      "      BatchNorm2d-89           [-1, 24, 14, 14]              48\n",
      "      BasicConv2d-90           [-1, 24, 14, 14]               0\n",
      "           Conv2d-91           [-1, 64, 14, 14]          13,824\n",
      "      BatchNorm2d-92           [-1, 64, 14, 14]             128\n",
      "      BasicConv2d-93           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-94          [-1, 512, 14, 14]               0\n",
      "           Conv2d-95           [-1, 64, 14, 14]          32,768\n",
      "      BatchNorm2d-96           [-1, 64, 14, 14]             128\n",
      "      BasicConv2d-97           [-1, 64, 14, 14]               0\n",
      "        Inception-98          [-1, 512, 14, 14]               0\n",
      "           Conv2d-99          [-1, 128, 14, 14]          65,536\n",
      "     BatchNorm2d-100          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-101          [-1, 128, 14, 14]               0\n",
      "          Conv2d-102          [-1, 128, 14, 14]          65,536\n",
      "     BatchNorm2d-103          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-104          [-1, 128, 14, 14]               0\n",
      "          Conv2d-105          [-1, 256, 14, 14]         294,912\n",
      "     BatchNorm2d-106          [-1, 256, 14, 14]             512\n",
      "     BasicConv2d-107          [-1, 256, 14, 14]               0\n",
      "          Conv2d-108           [-1, 24, 14, 14]          12,288\n",
      "     BatchNorm2d-109           [-1, 24, 14, 14]              48\n",
      "     BasicConv2d-110           [-1, 24, 14, 14]               0\n",
      "          Conv2d-111           [-1, 64, 14, 14]          13,824\n",
      "     BatchNorm2d-112           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-113           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-114          [-1, 512, 14, 14]               0\n",
      "          Conv2d-115           [-1, 64, 14, 14]          32,768\n",
      "     BatchNorm2d-116           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-117           [-1, 64, 14, 14]               0\n",
      "       Inception-118          [-1, 512, 14, 14]               0\n",
      "          Conv2d-119          [-1, 112, 14, 14]          57,344\n",
      "     BatchNorm2d-120          [-1, 112, 14, 14]             224\n",
      "     BasicConv2d-121          [-1, 112, 14, 14]               0\n",
      "          Conv2d-122          [-1, 144, 14, 14]          73,728\n",
      "     BatchNorm2d-123          [-1, 144, 14, 14]             288\n",
      "     BasicConv2d-124          [-1, 144, 14, 14]               0\n",
      "          Conv2d-125          [-1, 288, 14, 14]         373,248\n",
      "     BatchNorm2d-126          [-1, 288, 14, 14]             576\n",
      "     BasicConv2d-127          [-1, 288, 14, 14]               0\n",
      "          Conv2d-128           [-1, 32, 14, 14]          16,384\n",
      "     BatchNorm2d-129           [-1, 32, 14, 14]              64\n",
      "     BasicConv2d-130           [-1, 32, 14, 14]               0\n",
      "          Conv2d-131           [-1, 64, 14, 14]          18,432\n",
      "     BatchNorm2d-132           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-133           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-134          [-1, 512, 14, 14]               0\n",
      "          Conv2d-135           [-1, 64, 14, 14]          32,768\n",
      "     BatchNorm2d-136           [-1, 64, 14, 14]             128\n",
      "     BasicConv2d-137           [-1, 64, 14, 14]               0\n",
      "       Inception-138          [-1, 528, 14, 14]               0\n",
      "          Conv2d-139            [-1, 128, 4, 4]          67,584\n",
      "     BatchNorm2d-140            [-1, 128, 4, 4]             256\n",
      "     BasicConv2d-141            [-1, 128, 4, 4]               0\n",
      "          Linear-142                 [-1, 1024]       2,098,176\n",
      "          Linear-143                 [-1, 1000]       1,025,000\n",
      "    InceptionAux-144                 [-1, 1000]               0\n",
      "          Conv2d-145          [-1, 256, 14, 14]         135,168\n",
      "     BatchNorm2d-146          [-1, 256, 14, 14]             512\n",
      "     BasicConv2d-147          [-1, 256, 14, 14]               0\n",
      "          Conv2d-148          [-1, 160, 14, 14]          84,480\n",
      "     BatchNorm2d-149          [-1, 160, 14, 14]             320\n",
      "     BasicConv2d-150          [-1, 160, 14, 14]               0\n",
      "          Conv2d-151          [-1, 320, 14, 14]         460,800\n",
      "     BatchNorm2d-152          [-1, 320, 14, 14]             640\n",
      "     BasicConv2d-153          [-1, 320, 14, 14]               0\n",
      "          Conv2d-154           [-1, 32, 14, 14]          16,896\n",
      "     BatchNorm2d-155           [-1, 32, 14, 14]              64\n",
      "     BasicConv2d-156           [-1, 32, 14, 14]               0\n",
      "          Conv2d-157          [-1, 128, 14, 14]          36,864\n",
      "     BatchNorm2d-158          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-159          [-1, 128, 14, 14]               0\n",
      "       MaxPool2d-160          [-1, 528, 14, 14]               0\n",
      "          Conv2d-161          [-1, 128, 14, 14]          67,584\n",
      "     BatchNorm2d-162          [-1, 128, 14, 14]             256\n",
      "     BasicConv2d-163          [-1, 128, 14, 14]               0\n",
      "       Inception-164          [-1, 832, 14, 14]               0\n",
      "       MaxPool2d-165            [-1, 832, 7, 7]               0\n",
      "          Conv2d-166            [-1, 256, 7, 7]         212,992\n",
      "     BatchNorm2d-167            [-1, 256, 7, 7]             512\n",
      "     BasicConv2d-168            [-1, 256, 7, 7]               0\n",
      "          Conv2d-169            [-1, 160, 7, 7]         133,120\n",
      "     BatchNorm2d-170            [-1, 160, 7, 7]             320\n",
      "     BasicConv2d-171            [-1, 160, 7, 7]               0\n",
      "          Conv2d-172            [-1, 320, 7, 7]         460,800\n",
      "     BatchNorm2d-173            [-1, 320, 7, 7]             640\n",
      "     BasicConv2d-174            [-1, 320, 7, 7]               0\n",
      "          Conv2d-175             [-1, 32, 7, 7]          26,624\n",
      "     BatchNorm2d-176             [-1, 32, 7, 7]              64\n",
      "     BasicConv2d-177             [-1, 32, 7, 7]               0\n",
      "          Conv2d-178            [-1, 128, 7, 7]          36,864\n",
      "     BatchNorm2d-179            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-180            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-181            [-1, 832, 7, 7]               0\n",
      "          Conv2d-182            [-1, 128, 7, 7]         106,496\n",
      "     BatchNorm2d-183            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-184            [-1, 128, 7, 7]               0\n",
      "       Inception-185            [-1, 832, 7, 7]               0\n",
      "          Conv2d-186            [-1, 384, 7, 7]         319,488\n",
      "     BatchNorm2d-187            [-1, 384, 7, 7]             768\n",
      "     BasicConv2d-188            [-1, 384, 7, 7]               0\n",
      "          Conv2d-189            [-1, 192, 7, 7]         159,744\n",
      "     BatchNorm2d-190            [-1, 192, 7, 7]             384\n",
      "     BasicConv2d-191            [-1, 192, 7, 7]               0\n",
      "          Conv2d-192            [-1, 384, 7, 7]         663,552\n",
      "     BatchNorm2d-193            [-1, 384, 7, 7]             768\n",
      "     BasicConv2d-194            [-1, 384, 7, 7]               0\n",
      "          Conv2d-195             [-1, 48, 7, 7]          39,936\n",
      "     BatchNorm2d-196             [-1, 48, 7, 7]              96\n",
      "     BasicConv2d-197             [-1, 48, 7, 7]               0\n",
      "          Conv2d-198            [-1, 128, 7, 7]          55,296\n",
      "     BatchNorm2d-199            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-200            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-201            [-1, 832, 7, 7]               0\n",
      "          Conv2d-202            [-1, 128, 7, 7]         106,496\n",
      "     BatchNorm2d-203            [-1, 128, 7, 7]             256\n",
      "     BasicConv2d-204            [-1, 128, 7, 7]               0\n",
      "       Inception-205           [-1, 1024, 7, 7]               0\n",
      "AdaptiveAvgPool2d-206           [-1, 1024, 1, 1]               0\n",
      "         Dropout-207                 [-1, 1024]               0\n",
      "          Linear-208                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 13,004,888\n",
      "Trainable params: 13,004,888\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 94.25\n",
      "Params size (MB): 49.61\n",
      "Estimated Total Size (MB): 144.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Show and Tell: A Neural Image Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically describing the content of an image is a fundamental problem in AI that connects *computer vision* and *natural language processing*.\n",
    "In this task, we will be looking into how we can use CNNs and RNNs to build an Image Caption Generator.\n",
    "\n",
    "Specifically, you will be implementing and training the model [in this paper](https://arxiv.org/abs/1411.4555) with TensorFlow/Keras on one of the datasets mentioned in the paper.\n",
    "\n",
    "To lighten the burden on training the network, you can use any pretrained network in [tf.keras.applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoder\n",
    "First we fetch a pre-trained convolutional neural network and tweak it slightly so it's not focused on classification anymore. We'll use this to encode our image and then feed that to the caption generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Network to encode an image\"\"\"\n",
    "    def __init__(self, out_dim=1000):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.resnet = models.resnet152(pretrained=True)\n",
    "        # it's pretrained, so let's make it not change\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        # the last layer of resnet is Linear that is used for classification\n",
    "        # we'll change its size, mark it as learnable, and initialize it randomly, since it's not going to be classification anymore\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, out_dim) # constructing it marks it as learnable as well\n",
    "        assert(all(p.requires_grad for p in self.resnet.fc.parameters()))\n",
    "        self.resnet.fc.weight.data.normal_(0, 0.02) # weights to a small number\n",
    "        self.resnet.fc.bias.data.fill_(0) # bias to zero\n",
    "        \n",
    "        # TODO: maybe it also makes sense to tweak the one layer before that, that is, the pooling layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Generator\n",
    "This is a recurrent neural network that is first initialized with the encoded image and then starts generating words that will be used as the generated caption. The word embeddings are pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartiallyFixedEmbedding(nn.Module): # from https://discuss.pytorch.org/t/updating-part-of-an-embedding-matrix-only-for-out-of-vocab-words/33297/4\n",
    "    \"\"\"\n",
    "    This embedding has an embedding matrix that is split into two parts: fixed and variable.\n",
    "    The fixed part is not changed in the learning process, making it ideal for pretrained vocabularies with a few extra words that we want to learn.\n",
    "    \"\"\"\n",
    "    def __init__(self, fixed_weights, num_to_learn):\n",
    "        super().__init__()\n",
    "        self.num_fixed = fixed_weights.size(0)\n",
    "        self.num_to_learn = num_to_learn\n",
    "        weight = torch.empty(self.num_fixed + num_to_learn, fixed_weights.size(1))\n",
    "        weight[:self.num_fixed] = fixed_weights\n",
    "        self.trainable_weight = nn.Parameter(torch.empty(num_to_learn, fixed_weights.size(1)))\n",
    "        nn.init.kaiming_uniform_(self.trainable_weight)\n",
    "        weight[self.num_fixed:] = self.trainable_weight\n",
    "        self.register_buffer('weight', weight)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        self.weight.detach_()\n",
    "        self.weight[self.num_fixed:] = self.trainable_weight\n",
    "        return nn.functional.embedding(inp, self.weight, None, None, 2.0, False, False)\n",
    "\n",
    "    \n",
    "class Vocabulary(object):\n",
    "    START_TOKEN = '<start>'\n",
    "    END_TOKEN = '<end>'\n",
    "    PAD_TOKEN = '<pad>'\n",
    "    CONTROL_WORDS = [START_TOKEN, END_TOKEN, PAD_TOKEN]\n",
    "    \n",
    "    def __init__(self, non_control_words):\n",
    "        self.control_words_count = len(CaptionGenerator.CONTROL_WORDS)\n",
    "        \n",
    "        self.vocab = [w for w in non_control_words] + CaptionGenerator.CONTROL_WORDS\n",
    "        self.non_control_words = self.vocab[:-self.control_words_count]\n",
    "        self.word2idx = {w: i for (i, w) in enumerate(self.vocab)}\n",
    "        \n",
    "    def get_idx(self, word):\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def get_start_token_idx(self):\n",
    "        return self.word2idx[Vocabulary.START_TOKEN]\n",
    "    \n",
    "    def get_end_token_idx(self):\n",
    "        return self.word2idx[Vocabulary.END_TOKEN]\n",
    "    \n",
    "    def get_pad_token_idx(self):\n",
    "        return self.word2idx[Vocabulary.PAD_TOKEN]\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    \n",
    "class CaptionGenerator(nn.Module):\n",
    "    CONTROL_WORDS = ['<start>', '<end>', '<pad>']\n",
    "    \n",
    "    def __init__(self, vocab, word2vec, encoded_image_size, hidden_size=512, rnn_layers_num=1):\n",
    "        \"\"\"\n",
    "        :param vocab: set of all the words (as strings) in the vocabulary other than CONTROL_WORDS\n",
    "        :param word2vec: one of the pre-trained models from torchnlp library\n",
    "        \"\"\"\n",
    "        super(CaptionGenerator, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.embed = PartiallyFixedEmbedding(word2vec[vocab.non_control_words], vocab.control_words_count)\n",
    "        \n",
    "        self.initial_hidden_state = nn.Linear(encoded_image_size, hidden_size)\n",
    "        self.initial_cell_state = nn.Linear(encoded_image_size, hidden_size)\n",
    "        \n",
    "        self.recurrent_unit = nn.LSTM(word2vec.dim, hidden_size, rnn_layers_num, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, vocab.size())\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        embeddings = self.embed(captions)\n",
    "        inputs_packed = pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "        \n",
    "        initial_hidden = self.initial_hidden_state(features)\n",
    "        initial_hidden = initial_hidden.view(-1, initial_hidden.shape[0], initial_hidden.shape[1])\n",
    "        initial_cell = self.initial_cell_state(features)\n",
    "        initial_cell = initial_cell.view(-1, initial_cell.shape[0], initial_cell.shape[1])\n",
    "        \n",
    "        hiddens, _ = self.recurrent_unit(inputs_packed, (initial_hidden, initial_cell))\n",
    "        hiddens, _ = pad_packed_sequence(hiddens, batch_first=True)\n",
    "        outputs = self.linear(hiddens) # maybe a softmax after that?\n",
    "        return outputs\n",
    "    \n",
    "    def to_sentence(self, forward_out, lengths):\n",
    "        _, tops = forward_out.topk(1)\n",
    "        tops = tops.view(tops.shape[0], tops.shape[1])\n",
    "        sentences = []\n",
    "        for sentence in tops:\n",
    "            words = []\n",
    "            for idx in sentence:\n",
    "                words.append(self.vocab.vocab[idx])\n",
    "            sentences.append(' '.join(words))\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get some data. We downloaded and unpacked the Flickr8K dataset from http://academictorrents.com/details/9dea07ba660a722ae1008c4c8afdd303b6f6e53b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class Flickr8KDataset(data.Dataset):\n",
    "    def __init__(self, tokens_path, images_path):\n",
    "        self.img_names_and_captions = []\n",
    "        self.images_path = images_path\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)), # TODO: maybe add some cropping or sth? this will squash most of the images\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # normalization required by resnet\n",
    "        ])\n",
    "        with open(tokens_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                img_name = parts[0].split('#')[0] # each image has a number of captions - but we don't care how many\n",
    "                if not path.exists(path.join(self.images_path, img_name)):\n",
    "                    continue\n",
    "                tokens = parts[1:(-1 if parts[-1] == '.' else len(parts))] # tokens are everything apart from the image name and the final period\n",
    "                \n",
    "                self.img_names_and_captions.append((img_name, list(map(lambda t: t.lower(), tokens))))\n",
    "                \n",
    "        self.all_tokens = set()\n",
    "        for _, tokens in self.img_names_and_captions:\n",
    "            for token in tokens:\n",
    "                self.all_tokens.add(token)\n",
    "                \n",
    "        self.vocab = Vocabulary(self.all_tokens)\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.img_names_and_captions[idx]\n",
    "        img_path = path.join(self.images_path, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        caption = torch.tensor([self.vocab.get_start_token_idx()] + \n",
    "                               [self.vocab.get_idx(token) for token in caption] + \n",
    "                               [self.vocab.get_end_token_idx()])\n",
    "        return image, caption\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names_and_captions)\n",
    "    \n",
    "    \n",
    "def get_flickr8k_dataloader(tokens_path, images_path, batch_size=32, shuffle=True, num_workers=2):\n",
    "    flickr = Flickr8KDataset(tokens_path, images_path)\n",
    "    \n",
    "    def make_batch(data):\n",
    "        # sort data by caption length\n",
    "        data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "        images, captions = zip(*data)\n",
    "\n",
    "        # Merge image tensors (stack)\n",
    "        images = torch.stack(images, 0)\n",
    "\n",
    "        caption_lengths = [len(caption) for caption in captions]\n",
    "\n",
    "        # matrix full of the padding token index\n",
    "        padded_captions = torch.empty(len(captions), max(caption_lengths)).fill_(flickr.vocab.get_pad_token_idx()).long()\n",
    "\n",
    "        for i, caption in enumerate(captions):\n",
    "            end = caption_lengths[i]\n",
    "            padded_captions[i, :end] = caption[:end]\n",
    "        return images, padded_captions, caption_lengths\n",
    "    \n",
    "    return data.DataLoader(dataset=flickr,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=shuffle,\n",
    "                           num_workers=num_workers,\n",
    "                           collate_fn=make_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shapes of the data in one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images:  \t\ttorch.Size([32, 3, 224, 224])\n",
      "captions:\t\ttorch.Size([32, 23])\n",
      "lengths: \t\t[23, 19, 19, 18, 17, 17, 16, 15, 15, 14, 14, 14, 14, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 11, 11, 11, 9, 9, 9, 8, 6]\n",
      "onehot captions:\ttorch.Size([32, 23, 8921])\n"
     ]
    }
   ],
   "source": [
    "dl = get_flickr8k_dataloader('../../Flickr8k/Flickr8k_text/Flickr8k.token.txt', '../../Flickr8k/Flickr8k_Dataset/Flicker8k_Dataset')\n",
    "images, captions, lengths = next(iter(dl))\n",
    "print(f\"images:  \\t\\t{images.shape}\")\n",
    "print(f\"captions:\\t\\t{captions.shape}\")\n",
    "print(f\"lengths: \\t\\t{lengths}\")\n",
    "\n",
    "onehot_encoded = nn.functional.one_hot(captions, dl.dataset.vocab.size())\n",
    "print(f\"onehot captions:\\t{onehot_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check if the data can correctly flow through our networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded images: torch.Size([32, 1000])\n"
     ]
    }
   ],
   "source": [
    "image_encoding_length = 1000\n",
    "\n",
    "ie = ImageEncoder(image_encoding_length)\n",
    "features = ie.forward(images)\n",
    "print(f\"encoded images: {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated: torch.Size([32, 23, 8921])\n"
     ]
    }
   ],
   "source": [
    "from torchnlp.word_to_vector import GloVe\n",
    "\n",
    "vocab = dl.dataset.vocab\n",
    "cg = CaptionGenerator(vocab, GloVe(), image_encoding_length)\n",
    "\n",
    "generated = cg.forward(features, captions, lengths)\n",
    "print(f\"generated: {generated.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of generated sentences: 32\n",
      "here are some of them:\n",
      "lady lady pro 75 75 frizzy 75 75 75 barrior 75 paddle 32 75 frizzy 75 32 sad brightly-lit sad college college growling\n",
      "lasso pro 75 raked raked raked tourist tourist tourist exciting tourist tourist ghostbusters tourist 32 32 32 leashes tourist nametags nametags nametags nametags\n",
      "lasso antennae pro 32 75 illustration sad twenty racetrack tan-skinned tourist 32 barrior barrior barrior route 32 leashes leashes nametags nametags nametags nametags\n",
      "spin-art rush sad tucked floated juggles sad allowed arab 32 75 blowup fenced 32 pins so minding patrick nametags nametags nametags nametags nametags\n",
      "proof antennae agility maneuvers barrior barrior barrior 32 barrior barrior 75 barrior barrior reddish-brown 32 32 tourist nametags nametags nametags nametags nametags nametags\n"
     ]
    }
   ],
   "source": [
    "sentences = cg.to_sentence(generated, lengths)\n",
    "print(f\"number of generated sentences: {len(sentences)}\")\n",
    "newline = '\\n'\n",
    "print(f\"here are some of them:\\n{newline.join(sentences[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from datetime import datetime\n",
    "\n",
    "def train(num_epochs, data_loader, image_encoder, caption_generator):\n",
    "    optimizer = optim.Adam(params=\n",
    "                           list(filter(lambda p: p.requires_grad, image_encoder.parameters())) + \n",
    "                           list(filter(lambda p: p.requires_grad, caption_generator.parameters()))) # learning rate default for now\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        image_encoder.train()\n",
    "        caption_generator.train()\n",
    "        \n",
    "        for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "            t = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(f\"{t}\\tepoch {epoch}\\tbatch {i}\", end='\\t')\n",
    "            encoded_images = image_encoder(images)\n",
    "            outputs = caption_generator(encoded_images, captions, lengths)\n",
    "            \n",
    "            # since we want to predict the next word given a word, our target drops the initial <start> and adds an additional <pad>\n",
    "            pad = data_loader.dataset.vocab.get_pad_token_idx()\n",
    "            target = torch.cat((captions[:, 1:], torch.empty(captions.shape[0], 1).fill_(pad).long()), 1)\n",
    "            \n",
    "            # trick to flatten the data so it fits what the criterion expects\n",
    "            outputs, _, _, _ = pack_padded_sequence(outputs, lengths, batch_first=True)\n",
    "            target, _, _, _ = pack_padded_sequence(target, lengths, batch_first=True)\n",
    "            \n",
    "            loss = criterion(outputs, target)\n",
    "            print(f\"loss {loss.mean()}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:25:30\tepoch 0\tbatch 0\tloss 9.085869789123535\n",
      "21:25:35\tepoch 0\tbatch 1\tloss 8.868199348449707\n",
      "21:25:40\tepoch 0\tbatch 2\tloss 8.488537788391113\n",
      "21:25:46\tepoch 0\tbatch 3\tloss 8.096234321594238\n",
      "21:25:53\tepoch 0\tbatch 4\tloss 7.648326873779297\n",
      "21:25:59\tepoch 0\tbatch 5\tloss 7.111017227172852\n",
      "21:26:05\tepoch 0\tbatch 6\tloss 6.718966960906982\n",
      "21:26:12\tepoch 0\tbatch 7\tloss 6.282887935638428\n",
      "21:26:18\tepoch 0\tbatch 8\tloss 5.889003276824951\n",
      "21:26:24\tepoch 0\tbatch 9\tloss 5.726478099822998\n",
      "21:26:30\tepoch 0\tbatch 10\tloss 5.421303749084473\n",
      "21:26:36\tepoch 0\tbatch 11\tloss 5.272465705871582\n",
      "21:26:42\tepoch 0\tbatch 12\tloss 5.234569549560547\n",
      "21:26:48\tepoch 0\tbatch 13\tloss 5.1503586769104\n",
      "21:26:55\tepoch 0\tbatch 14\tloss 5.357510089874268\n",
      "21:27:02\tepoch 0\tbatch 15\tloss 5.61923360824585\n",
      "21:27:09\tepoch 0\tbatch 16\tloss 5.460638046264648\n",
      "21:27:16\tepoch 0\tbatch 17\tloss 5.451900959014893\n",
      "21:27:22\tepoch 0\tbatch 18\tloss 5.433993816375732\n",
      "21:27:29\tepoch 0\tbatch 19\tloss 5.260509490966797\n",
      "21:27:36\tepoch 0\tbatch 20\tloss 5.408246040344238\n",
      "21:27:41\tepoch 0\tbatch 21\tloss 5.285902976989746\n",
      "21:27:47\tepoch 0\tbatch 22\tloss 5.313559532165527\n",
      "21:27:54\tepoch 0\tbatch 23\tloss 4.927196502685547\n",
      "21:28:00\tepoch 0\tbatch 24\tloss 5.080047130584717\n",
      "21:28:06\tepoch 0\tbatch 25\tloss 5.013794422149658\n",
      "21:28:12\tepoch 0\tbatch 26\tloss 5.254959583282471\n",
      "21:28:17\tepoch 0\tbatch 27\tloss 4.87755823135376\n",
      "21:28:23\tepoch 0\tbatch 28\tloss 5.206336975097656\n",
      "21:28:29\tepoch 0\tbatch 29\tloss 5.284421920776367\n",
      "21:28:34\tepoch 0\tbatch 30\tloss 5.080692291259766\n",
      "21:28:40\tepoch 0\tbatch 31\tloss 5.102403163909912\n",
      "21:28:45\tepoch 0\tbatch 32\tloss 5.105689525604248\n",
      "21:28:51\tepoch 0\tbatch 33\tloss 4.980501651763916\n",
      "21:28:57\tepoch 0\tbatch 34\tloss 5.3096795082092285\n",
      "21:29:02\tepoch 0\tbatch 35\tloss 5.012578010559082\n",
      "21:29:08\tepoch 0\tbatch 36\tloss 5.0175557136535645\n",
      "21:29:14\tepoch 0\tbatch 37\tloss 5.050196647644043\n",
      "21:29:19\tepoch 0\tbatch 38\tloss 4.944149017333984\n",
      "21:29:25\tepoch 0\tbatch 39\tloss 4.954481601715088\n",
      "21:29:30\tepoch 0\tbatch 40\tloss 4.975258827209473\n",
      "21:29:35\tepoch 0\tbatch 41\tloss 4.940851211547852\n",
      "21:29:41\tepoch 0\tbatch 42\tloss 4.860349655151367\n",
      "21:29:46\tepoch 0\tbatch 43\tloss 4.833566188812256\n",
      "21:29:52\tepoch 0\tbatch 44\tloss 5.003981590270996\n",
      "21:29:57\tepoch 0\tbatch 45\tloss 4.814579010009766\n",
      "21:30:03\tepoch 0\tbatch 46\tloss 4.766057968139648\n",
      "21:30:08\tepoch 0\tbatch 47\tloss 4.760797023773193\n",
      "21:30:14\tepoch 0\tbatch 48\tloss 4.527857780456543\n",
      "21:30:20\tepoch 0\tbatch 49\tloss 4.6654534339904785\n",
      "21:30:25\tepoch 0\tbatch 50\tloss 4.627101421356201\n",
      "21:30:31\tepoch 0\tbatch 51\tloss 4.727999687194824\n",
      "21:30:36\tepoch 0\tbatch 52\tloss 4.799780368804932\n",
      "21:30:42\tepoch 0\tbatch 53\tloss 4.7114996910095215\n",
      "21:30:48\tepoch 0\tbatch 54\tloss 4.788405418395996\n",
      "21:30:53\tepoch 0\tbatch 55\tloss 4.657489776611328\n",
      "21:30:59\tepoch 0\tbatch 56\tloss 4.4044880867004395\n",
      "21:31:04\tepoch 0\tbatch 57\tloss 4.744065284729004\n",
      "21:31:10\tepoch 0\tbatch 58\tloss 4.765063285827637\n",
      "21:31:16\tepoch 0\tbatch 59\tloss 4.768873691558838\n",
      "21:31:21\tepoch 0\tbatch 60\tloss 4.479975700378418\n",
      "21:31:27\tepoch 0\tbatch 61\tloss 4.588491916656494\n",
      "21:31:32\tepoch 0\tbatch 62\tloss 4.540676116943359\n",
      "21:31:38\tepoch 0\tbatch 63\tloss 4.7117533683776855\n",
      "21:31:43\tepoch 0\tbatch 64\tloss 4.381628036499023\n",
      "21:31:49\tepoch 0\tbatch 65\tloss 4.76292610168457\n",
      "21:31:55\tepoch 0\tbatch 66\tloss 4.683370590209961\n",
      "21:32:00\tepoch 0\tbatch 67\tloss 4.4688920974731445\n",
      "21:32:06\tepoch 0\tbatch 68\tloss 4.357224464416504\n",
      "21:32:11\tepoch 0\tbatch 69\tloss 4.4374284744262695\n",
      "21:32:17\tepoch 0\tbatch 70\tloss 4.22967004776001\n",
      "21:32:22\tepoch 0\tbatch 71\tloss 4.64080286026001\n",
      "21:32:28\tepoch 0\tbatch 72\tloss 4.754743576049805\n",
      "21:32:33\tepoch 0\tbatch 73\tloss 4.502653121948242\n",
      "21:32:39\tepoch 0\tbatch 74\tloss 4.381507873535156\n",
      "21:32:45\tepoch 0\tbatch 75\tloss 4.399542808532715\n",
      "21:32:50\tepoch 0\tbatch 76\tloss 4.3820319175720215\n",
      "21:32:56\tepoch 0\tbatch 77\tloss 4.553040504455566\n",
      "21:33:01\tepoch 0\tbatch 78\tloss 4.5440545082092285\n",
      "21:33:07\tepoch 0\tbatch 79\tloss 4.382328510284424\n",
      "21:33:12\tepoch 0\tbatch 80\tloss 4.145900726318359\n",
      "21:33:18\tepoch 0\tbatch 81\tloss 4.255484580993652\n",
      "21:33:23\tepoch 0\tbatch 82\tloss 4.492413520812988\n",
      "21:33:29\tepoch 0\tbatch 83\tloss 4.533699035644531\n",
      "21:33:34\tepoch 0\tbatch 84\tloss 4.785105228424072\n",
      "21:33:40\tepoch 0\tbatch 85\tloss 4.242594242095947\n",
      "21:33:45\tepoch 0\tbatch 86\tloss 4.533328056335449\n",
      "21:33:51\tepoch 0\tbatch 87\tloss 4.248952865600586\n",
      "21:33:57\tepoch 0\tbatch 88\tloss 4.48350715637207\n",
      "21:34:02\tepoch 0\tbatch 89\tloss 4.501918315887451\n",
      "21:34:08\tepoch 0\tbatch 90\tloss 4.2437028884887695\n",
      "21:34:13\tepoch 0\tbatch 91\tloss 4.252532005310059\n",
      "21:34:19\tepoch 0\tbatch 92\tloss 4.682441711425781\n",
      "21:34:24\tepoch 0\tbatch 93\tloss 4.406700611114502\n",
      "21:34:30\tepoch 0\tbatch 94\tloss 4.3607025146484375\n",
      "21:34:36\tepoch 0\tbatch 95\tloss 4.179649829864502\n",
      "21:34:41\tepoch 0\tbatch 96\tloss 4.2850141525268555\n",
      "21:34:47\tepoch 0\tbatch 97\tloss 4.371903896331787\n",
      "21:34:52\tepoch 0\tbatch 98\tloss 4.3599114418029785\n",
      "21:34:58\tepoch 0\tbatch 99\tloss 4.42474365234375\n",
      "21:35:03\tepoch 0\tbatch 100\tloss 4.155756950378418\n",
      "21:35:09\tepoch 0\tbatch 101\tloss 4.106457710266113\n",
      "21:35:15\tepoch 0\tbatch 102\tloss 4.189976215362549\n",
      "21:35:20\tepoch 0\tbatch 103\tloss 4.156290531158447\n",
      "21:35:26\tepoch 0\tbatch 104\tloss 4.096048355102539\n",
      "21:35:31\tepoch 0\tbatch 105\tloss 3.9448466300964355\n",
      "21:35:37\tepoch 0\tbatch 106\tloss 4.344397068023682\n",
      "21:35:43\tepoch 0\tbatch 107\tloss 4.558089733123779\n",
      "21:35:48\tepoch 0\tbatch 108\tloss 4.194757461547852\n",
      "21:35:54\tepoch 0\tbatch 109\tloss 4.074412822723389\n",
      "21:36:00\tepoch 0\tbatch 110\tloss 4.101205348968506\n",
      "21:36:05\tepoch 0\tbatch 111\tloss 4.254933834075928\n",
      "21:36:11\tepoch 0\tbatch 112\tloss 4.031985759735107\n",
      "21:36:17\tepoch 0\tbatch 113\tloss 4.338532447814941\n",
      "21:36:22\tepoch 0\tbatch 114\tloss 4.028188705444336\n",
      "21:36:28\tepoch 0\tbatch 115\tloss 3.939168691635132\n",
      "21:36:34\tepoch 0\tbatch 116\tloss 4.425165176391602\n",
      "21:36:39\tepoch 0\tbatch 117\tloss 4.309611797332764\n",
      "21:36:45\tepoch 0\tbatch 118\tloss 4.102991104125977\n",
      "21:36:50\tepoch 0\tbatch 119\tloss 4.0625996589660645\n",
      "21:36:56\tepoch 0\tbatch 120\tloss 4.20486307144165\n",
      "21:37:01\tepoch 0\tbatch 121\tloss 4.18654203414917\n",
      "21:37:07\tepoch 0\tbatch 122\tloss 4.129297733306885\n",
      "21:37:13\tepoch 0\tbatch 123\tloss 3.8898274898529053\n",
      "21:37:18\tepoch 0\tbatch 124\tloss 3.8743598461151123\n",
      "21:37:24\tepoch 0\tbatch 125\tloss 4.2934722900390625\n",
      "21:37:29\tepoch 0\tbatch 126\tloss 4.026994228363037\n",
      "21:37:35\tepoch 0\tbatch 127\tloss 4.29414176940918\n",
      "21:37:41\tepoch 0\tbatch 128\tloss 4.397837162017822\n",
      "21:37:46\tepoch 0\tbatch 129\tloss 4.086269378662109\n",
      "21:37:52\tepoch 0\tbatch 130\tloss 4.071175575256348\n",
      "21:37:58\tepoch 0\tbatch 131\tloss 4.271679401397705\n",
      "21:38:04\tepoch 0\tbatch 132\tloss 4.050432205200195\n",
      "21:38:10\tepoch 0\tbatch 133\tloss 3.9157826900482178\n",
      "21:38:16\tepoch 0\tbatch 134\tloss 4.060783863067627\n",
      "21:38:22\tepoch 0\tbatch 135\tloss 3.9404730796813965\n",
      "21:38:27\tepoch 0\tbatch 136\tloss 4.091320514678955\n",
      "21:38:33\tepoch 0\tbatch 137\tloss 4.255877494812012\n",
      "21:38:38\tepoch 0\tbatch 138\tloss 4.165670871734619\n",
      "21:38:44\tepoch 0\tbatch 139\tloss 4.127066612243652\n",
      "21:38:50\tepoch 0\tbatch 140\tloss 4.35684871673584\n",
      "21:38:55\tepoch 0\tbatch 141\tloss 4.226341724395752\n",
      "21:39:01\tepoch 0\tbatch 142\tloss 4.120966911315918\n",
      "21:39:07\tepoch 0\tbatch 143\tloss 4.252971172332764\n",
      "21:39:12\tepoch 0\tbatch 144\tloss 4.07958459854126\n",
      "21:39:18\tepoch 0\tbatch 145\tloss 4.254028797149658\n",
      "21:39:24\tepoch 0\tbatch 146\tloss 4.040963172912598\n",
      "21:39:29\tepoch 0\tbatch 147\tloss 3.9377381801605225\n",
      "21:39:35\tepoch 0\tbatch 148\tloss 4.276917934417725\n",
      "21:39:41\tepoch 0\tbatch 149\tloss 4.164577960968018\n",
      "21:39:47\tepoch 0\tbatch 150\tloss 3.771587610244751\n",
      "21:39:53\tepoch 0\tbatch 151\tloss 4.2072553634643555\n",
      "21:39:58\tepoch 0\tbatch 152\tloss 4.2518720626831055\n",
      "21:40:04\tepoch 0\tbatch 153\tloss 4.021608829498291\n",
      "21:40:10\tepoch 0\tbatch 154\tloss 4.065834999084473\n",
      "21:40:15\tepoch 0\tbatch 155\tloss 4.001129150390625\n",
      "21:40:21\tepoch 0\tbatch 156\tloss 3.9080097675323486\n",
      "21:40:27\tepoch 0\tbatch 157\tloss 3.7225236892700195\n",
      "21:40:33\tepoch 0\tbatch 158\tloss 4.013561248779297\n",
      "21:40:38\tepoch 0\tbatch 159\tloss 4.21497106552124\n",
      "21:40:44\tepoch 0\tbatch 160\tloss 4.165031433105469\n",
      "21:40:50\tepoch 0\tbatch 161\tloss 3.643796920776367\n",
      "21:40:55\tepoch 0\tbatch 162\tloss 4.2619709968566895\n",
      "21:41:01\tepoch 0\tbatch 163\tloss 3.7115917205810547\n",
      "21:41:06\tepoch 0\tbatch 164\tloss 3.962960958480835\n",
      "21:41:12\tepoch 0\tbatch 165\tloss 4.317326545715332\n",
      "21:41:18\tepoch 0\tbatch 166\tloss 4.081133842468262\n",
      "21:41:23\tepoch 0\tbatch 167\tloss 3.7302026748657227\n",
      "21:41:29\tepoch 0\tbatch 168\tloss 3.9849069118499756\n",
      "21:41:35\tepoch 0\tbatch 169\tloss 4.029470920562744\n",
      "21:41:40\tepoch 0\tbatch 170\tloss 4.187811374664307\n",
      "21:41:46\tepoch 0\tbatch 171\tloss 4.145174503326416\n",
      "21:41:52\tepoch 0\tbatch 172\tloss 4.094034194946289\n",
      "21:41:57\tepoch 0\tbatch 173\tloss 3.7750680446624756\n",
      "21:42:03\tepoch 0\tbatch 174\tloss 4.163643836975098\n",
      "21:42:08\tepoch 0\tbatch 175\tloss 3.9868359565734863\n",
      "21:42:14\tepoch 0\tbatch 176\tloss 4.19556188583374\n",
      "21:42:19\tepoch 0\tbatch 177\tloss 3.629465103149414\n",
      "21:42:25\tepoch 0\tbatch 178\tloss 3.8073713779449463\n",
      "21:42:31\tepoch 0\tbatch 179\tloss 3.8702359199523926\n",
      "21:42:36\tepoch 0\tbatch 180\tloss 3.8082637786865234\n",
      "21:42:43\tepoch 0\tbatch 181\tloss 3.913254499435425\n",
      "21:42:48\tepoch 0\tbatch 182\tloss 3.7475149631500244\n",
      "21:42:54\tepoch 0\tbatch 183\tloss 3.8685524463653564\n",
      "21:42:59\tepoch 0\tbatch 184\tloss 4.093664646148682\n",
      "21:43:05\tepoch 0\tbatch 185\tloss 3.6248526573181152\n",
      "21:43:11\tepoch 0\tbatch 186\tloss 4.024595737457275\n",
      "21:43:17\tepoch 0\tbatch 187\tloss 3.9044976234436035\n",
      "21:43:22\tepoch 0\tbatch 188\tloss 4.130725383758545\n",
      "21:43:28\tepoch 0\tbatch 189\tloss 3.8223254680633545\n",
      "21:43:33\tepoch 0\tbatch 190\tloss 4.062714576721191\n",
      "21:43:39\tepoch 0\tbatch 191\tloss 3.7235190868377686\n",
      "21:43:45\tepoch 0\tbatch 192\tloss 3.8232240676879883\n",
      "21:43:50\tepoch 0\tbatch 193\tloss 3.702869415283203\n",
      "21:43:56\tepoch 0\tbatch 194\tloss 3.97744083404541\n",
      "21:44:01\tepoch 0\tbatch 195\tloss 3.8397974967956543\n",
      "21:44:07\tepoch 0\tbatch 196\tloss 3.824038028717041\n",
      "21:44:12\tepoch 0\tbatch 197\tloss 3.6796000003814697\n",
      "21:44:18\tepoch 0\tbatch 198\tloss 3.9604299068450928\n",
      "21:44:24\tepoch 0\tbatch 199\tloss 3.710735321044922\n",
      "21:44:29\tepoch 0\tbatch 200\tloss 3.8555305004119873\n",
      "21:44:35\tepoch 0\tbatch 201\tloss 3.7675554752349854\n",
      "21:44:40\tepoch 0\tbatch 202\tloss 3.923509359359741\n",
      "21:44:46\tepoch 0\tbatch 203\tloss 3.7585289478302\n",
      "21:44:51\tepoch 0\tbatch 204\tloss 3.959301233291626\n",
      "21:44:57\tepoch 0\tbatch 205\tloss 3.7625021934509277\n",
      "21:45:02\tepoch 0\tbatch 206\tloss 3.7384772300720215\n",
      "21:45:08\tepoch 0\tbatch 207\tloss 3.625551462173462\n",
      "21:45:14\tepoch 0\tbatch 208\tloss 3.521151542663574\n",
      "21:45:19\tepoch 0\tbatch 209\tloss 3.8675451278686523\n",
      "21:45:25\tepoch 0\tbatch 210\tloss 4.0828471183776855\n",
      "21:45:30\tepoch 0\tbatch 211\tloss 3.945889472961426\n",
      "21:45:36\tepoch 0\tbatch 212\tloss 4.009214401245117\n",
      "21:45:42\tepoch 0\tbatch 213\tloss 4.107559680938721\n",
      "21:45:48\tepoch 0\tbatch 214\tloss 3.6997828483581543\n",
      "21:45:53\tepoch 0\tbatch 215\tloss 3.9456746578216553\n",
      "21:45:59\tepoch 0\tbatch 216\tloss 3.9725615978240967\n",
      "21:46:05\tepoch 0\tbatch 217\tloss 4.243192195892334\n",
      "21:46:11\tepoch 0\tbatch 218\tloss 3.5959231853485107\n",
      "21:46:16\tepoch 0\tbatch 219\tloss 3.7442867755889893\n",
      "21:46:22\tepoch 0\tbatch 220\tloss 3.9898972511291504\n",
      "21:46:28\tepoch 0\tbatch 221\tloss 3.87427020072937\n",
      "21:46:34\tepoch 0\tbatch 222\tloss 4.085644245147705\n",
      "21:46:40\tepoch 0\tbatch 223\tloss 3.7703680992126465\n",
      "21:46:45\tepoch 0\tbatch 224\tloss 3.8852596282958984\n",
      "21:46:51\tepoch 0\tbatch 225\tloss 3.8936755657196045\n",
      "21:46:57\tepoch 0\tbatch 226\tloss 3.518765449523926\n",
      "21:47:02\tepoch 0\tbatch 227\tloss 3.822970390319824\n",
      "21:47:08\tepoch 0\tbatch 228\tloss 3.5470714569091797\n",
      "21:47:14\tepoch 0\tbatch 229\tloss 3.787642478942871\n",
      "21:47:20\tepoch 0\tbatch 230\tloss 3.555346727371216\n",
      "21:47:25\tepoch 0\tbatch 231\tloss 3.902432680130005\n",
      "21:47:31\tepoch 0\tbatch 232\tloss 3.6808228492736816\n",
      "21:47:37\tepoch 0\tbatch 233\tloss 3.794935941696167\n",
      "21:47:43\tepoch 0\tbatch 234\tloss 3.845557928085327\n",
      "21:47:48\tepoch 0\tbatch 235\tloss 3.7615201473236084\n",
      "21:47:54\tepoch 0\tbatch 236\tloss 3.9998700618743896\n",
      "21:48:00\tepoch 0\tbatch 237\tloss 3.8963959217071533\n",
      "21:48:05\tepoch 0\tbatch 238\tloss 3.9883086681365967\n",
      "21:48:12\tepoch 0\tbatch 239\tloss 3.809454917907715\n",
      "21:48:18\tepoch 0\tbatch 240\tloss 4.155333518981934\n",
      "21:48:23\tepoch 0\tbatch 241\tloss 3.8891096115112305\n",
      "21:48:29\tepoch 0\tbatch 242\tloss 3.5354321002960205\n",
      "21:48:35\tepoch 0\tbatch 243\tloss 3.829343557357788\n",
      "21:48:41\tepoch 0\tbatch 244\tloss 3.6382498741149902\n",
      "21:48:47\tepoch 0\tbatch 245\tloss 3.365358352661133\n",
      "21:48:53\tepoch 0\tbatch 246\tloss 3.8999621868133545\n",
      "21:48:58\tepoch 0\tbatch 247\tloss 3.779550075531006\n",
      "21:49:04\tepoch 0\tbatch 248\tloss 3.8464255332946777\n",
      "21:49:10\tepoch 0\tbatch 249\tloss 3.686136484146118\n",
      "21:49:15\tepoch 0\tbatch 250\tloss 3.7015459537506104\n",
      "21:49:21\tepoch 0\tbatch 251\tloss 3.967198610305786\n",
      "21:49:27\tepoch 0\tbatch 252\tloss 3.8493103981018066\n",
      "21:49:33\tepoch 0\tbatch 253\tloss 3.841813802719116\n",
      "21:49:38\tepoch 0\tbatch 254\tloss 3.6973490715026855\n",
      "21:49:44\tepoch 0\tbatch 255\tloss 3.6601550579071045\n",
      "21:49:50\tepoch 0\tbatch 256\tloss 3.824192762374878\n",
      "21:49:56\tepoch 0\tbatch 257\tloss 3.865245819091797\n",
      "21:50:02\tepoch 0\tbatch 258\tloss 3.6781327724456787\n",
      "21:50:07\tepoch 0\tbatch 259\tloss 3.821436643600464\n",
      "21:50:13\tepoch 0\tbatch 260\tloss 3.5475447177886963\n",
      "21:50:19\tepoch 0\tbatch 261\tloss 4.095088005065918\n",
      "21:50:24\tepoch 0\tbatch 262\tloss 3.591291904449463\n",
      "21:50:30\tepoch 0\tbatch 263\tloss 3.849976062774658\n",
      "21:50:36\tepoch 0\tbatch 264\tloss 3.583836317062378\n",
      "21:50:42\tepoch 0\tbatch 265\tloss 3.783724069595337\n",
      "21:50:47\tepoch 0\tbatch 266\tloss 3.563460111618042\n",
      "21:50:53\tepoch 0\tbatch 267\tloss 3.9574646949768066\n",
      "21:50:59\tepoch 0\tbatch 268\tloss 3.480281352996826\n",
      "21:51:05\tepoch 0\tbatch 269\tloss 3.6799654960632324\n",
      "21:51:11\tepoch 0\tbatch 270\tloss 3.7498579025268555\n",
      "21:51:16\tepoch 0\tbatch 271\tloss 3.6776058673858643\n",
      "21:51:22\tepoch 0\tbatch 272\tloss 4.025402069091797\n",
      "21:51:28\tepoch 0\tbatch 273\tloss 3.7164340019226074\n",
      "21:51:33\tepoch 0\tbatch 274\tloss 3.9154932498931885\n",
      "21:51:39\tepoch 0\tbatch 275\tloss 3.545790910720825\n",
      "21:51:45\tepoch 0\tbatch 276\tloss 3.8273026943206787\n",
      "21:51:50\tepoch 0\tbatch 277\tloss 3.6996383666992188\n",
      "21:51:56\tepoch 0\tbatch 278\tloss 3.7719943523406982\n",
      "21:52:02\tepoch 0\tbatch 279\tloss 3.717984676361084\n",
      "21:52:08\tepoch 0\tbatch 280\tloss 3.7401959896087646\n",
      "21:52:13\tepoch 0\tbatch 281\tloss 4.022192001342773\n",
      "21:52:19\tepoch 0\tbatch 282\tloss 3.720186710357666\n",
      "21:52:25\tepoch 0\tbatch 283\tloss 3.637345552444458\n",
      "21:52:31\tepoch 0\tbatch 284\tloss 3.4533095359802246\n",
      "21:52:37\tepoch 0\tbatch 285\tloss 3.6487059593200684\n",
      "21:52:42\tepoch 0\tbatch 286\tloss 3.8130621910095215\n",
      "21:52:48\tepoch 0\tbatch 287\tloss 3.3878605365753174\n",
      "21:52:54\tepoch 0\tbatch 288\tloss 3.9166274070739746\n",
      "21:52:59\tepoch 0\tbatch 289\tloss 3.8340938091278076\n",
      "21:53:05\tepoch 0\tbatch 290\tloss 3.780385971069336\n",
      "21:53:11\tepoch 0\tbatch 291\tloss 3.5588011741638184\n",
      "21:53:17\tepoch 0\tbatch 292\tloss 3.602998971939087\n",
      "21:53:22\tepoch 0\tbatch 293\tloss 3.89566969871521\n",
      "21:53:28\tepoch 0\tbatch 294\tloss 3.5841803550720215\n",
      "21:53:34\tepoch 0\tbatch 295\tloss 3.818493127822876\n",
      "21:53:40\tepoch 0\tbatch 296\tloss 3.6866633892059326\n",
      "21:53:46\tepoch 0\tbatch 297\tloss 3.6847500801086426\n",
      "21:53:51\tepoch 0\tbatch 298\tloss 3.4932680130004883\n",
      "21:53:57\tepoch 0\tbatch 299\tloss 3.6846539974212646\n",
      "21:54:03\tepoch 0\tbatch 300\tloss 3.7442805767059326\n",
      "21:54:09\tepoch 0\tbatch 301\tloss 3.4942634105682373\n",
      "21:54:14\tepoch 0\tbatch 302\tloss 3.696852207183838\n",
      "21:54:20\tepoch 0\tbatch 303\tloss 3.864091634750366\n",
      "21:54:26\tepoch 0\tbatch 304\tloss 3.582730770111084\n",
      "21:54:32\tepoch 0\tbatch 305\tloss 3.6056034564971924\n",
      "21:54:37\tepoch 0\tbatch 306\tloss 3.53607439994812\n",
      "21:54:43\tepoch 0\tbatch 307\tloss 3.668700933456421\n",
      "21:54:49\tepoch 0\tbatch 308\tloss 3.397691488265991\n",
      "21:54:55\tepoch 0\tbatch 309\tloss 3.8033370971679688\n",
      "21:55:00\tepoch 0\tbatch 310\tloss 3.5373587608337402\n",
      "21:55:06\tepoch 0\tbatch 311\tloss 3.4931185245513916\n",
      "21:55:12\tepoch 0\tbatch 312\tloss 3.997596502304077\n",
      "21:55:18\tepoch 0\tbatch 313\tloss 3.9244253635406494\n",
      "21:55:24\tepoch 0\tbatch 314\tloss 3.5763132572174072\n",
      "21:55:29\tepoch 0\tbatch 315\tloss 3.691330671310425\n",
      "21:55:35\tepoch 0\tbatch 316\tloss 3.8262078762054443\n",
      "21:55:41\tepoch 0\tbatch 317\tloss 3.6927740573883057\n",
      "21:55:47\tepoch 0\tbatch 318\tloss 3.769761085510254\n",
      "21:55:52\tepoch 0\tbatch 319\tloss 3.5465943813323975\n",
      "21:55:58\tepoch 0\tbatch 320\tloss 3.791124105453491\n",
      "21:56:04\tepoch 0\tbatch 321\tloss 3.1664204597473145\n",
      "21:56:10\tepoch 0\tbatch 322\tloss 3.578800916671753\n",
      "21:56:16\tepoch 0\tbatch 323\tloss 3.7256503105163574\n",
      "21:56:22\tepoch 0\tbatch 324\tloss 3.5571975708007812\n",
      "21:56:27\tepoch 0\tbatch 325\tloss 3.5721614360809326\n",
      "21:56:33\tepoch 0\tbatch 326\tloss 3.756732702255249\n",
      "21:56:39\tepoch 0\tbatch 327\tloss 3.667144536972046\n",
      "21:56:45\tepoch 0\tbatch 328\tloss 3.8746063709259033\n",
      "21:56:51\tepoch 0\tbatch 329\tloss 3.6321091651916504\n",
      "21:56:56\tepoch 0\tbatch 330\tloss 3.750932216644287\n",
      "21:57:02\tepoch 0\tbatch 331\tloss 3.6779561042785645\n",
      "21:57:08\tepoch 0\tbatch 332\tloss 3.5198867321014404\n",
      "21:57:13\tepoch 0\tbatch 333\tloss 3.7085025310516357\n",
      "21:57:19\tepoch 0\tbatch 334\tloss 3.72478985786438\n",
      "21:57:25\tepoch 0\tbatch 335\tloss 3.379396915435791\n",
      "21:57:31\tepoch 0\tbatch 336\tloss 3.533432960510254\n",
      "21:57:37\tepoch 0\tbatch 337\tloss 3.4543161392211914\n",
      "21:57:42\tepoch 0\tbatch 338\tloss 3.869838237762451\n",
      "21:57:48\tepoch 0\tbatch 339\tloss 3.548685073852539\n",
      "21:57:54\tepoch 0\tbatch 340\tloss 3.388000726699829\n",
      "21:58:00\tepoch 0\tbatch 341\tloss 3.6701645851135254\n",
      "21:58:05\tepoch 0\tbatch 342\tloss 3.4412689208984375\n",
      "21:58:11\tepoch 0\tbatch 343\tloss 3.6860063076019287\n",
      "21:58:17\tepoch 0\tbatch 344\tloss 3.609067440032959\n",
      "21:58:23\tepoch 0\tbatch 345\tloss 3.2961111068725586\n",
      "21:58:28\tepoch 0\tbatch 346\tloss 3.7072062492370605\n",
      "21:58:34\tepoch 0\tbatch 347\tloss 3.6009979248046875\n",
      "21:58:41\tepoch 0\tbatch 348\tloss 3.6585543155670166\n",
      "21:58:47\tepoch 0\tbatch 349\tloss 3.4555230140686035\n",
      "21:58:53\tepoch 0\tbatch 350\tloss 3.4586007595062256\n",
      "21:58:59\tepoch 0\tbatch 351\tloss 3.62695050239563\n",
      "21:59:06\tepoch 0\tbatch 352\tloss 3.6622684001922607\n",
      "21:59:11\tepoch 0\tbatch 353\tloss 3.5684492588043213\n",
      "21:59:17\tepoch 0\tbatch 354\tloss 3.6697769165039062\n",
      "21:59:23\tepoch 0\tbatch 355\tloss 3.587843179702759\n",
      "21:59:29\tepoch 0\tbatch 356\tloss 3.796064615249634\n",
      "21:59:35\tepoch 0\tbatch 357\tloss 3.5690391063690186\n",
      "21:59:42\tepoch 0\tbatch 358\tloss 3.7379298210144043\n",
      "21:59:48\tepoch 0\tbatch 359\tloss 3.4037728309631348\n",
      "21:59:54\tepoch 0\tbatch 360\tloss 3.6355717182159424\n",
      "22:00:01\tepoch 0\tbatch 361\tloss 3.751584053039551\n",
      "22:00:08\tepoch 0\tbatch 362\tloss 3.7693591117858887\n",
      "22:00:14\tepoch 0\tbatch 363\tloss 3.710777521133423\n",
      "22:00:20\tepoch 0\tbatch 364\tloss 3.689924955368042\n",
      "22:00:27\tepoch 0\tbatch 365\tloss 3.4483282566070557\n",
      "22:00:33\tepoch 0\tbatch 366\tloss 3.462599754333496\n",
      "22:00:40\tepoch 0\tbatch 367\tloss 3.3518106937408447\n",
      "22:00:46\tepoch 0\tbatch 368\tloss 3.1921989917755127\n",
      "22:00:51\tepoch 0\tbatch 369\tloss 3.115854501724243\n",
      "22:00:57\tepoch 0\tbatch 370\tloss 3.544881820678711\n",
      "22:01:03\tepoch 0\tbatch 371\tloss 3.6753358840942383\n",
      "22:01:10\tepoch 0\tbatch 372\tloss 4.00885534286499\n",
      "22:01:16\tepoch 0\tbatch 373\tloss 3.5265212059020996\n",
      "22:01:23\tepoch 0\tbatch 374\tloss 3.606464385986328\n",
      "22:01:30\tepoch 0\tbatch 375\tloss 3.566898822784424\n",
      "22:01:37\tepoch 0\tbatch 376\tloss 3.4215519428253174\n",
      "22:01:43\tepoch 0\tbatch 377\tloss 3.623652458190918\n",
      "22:01:50\tepoch 0\tbatch 378\tloss 3.4018826484680176\n",
      "22:01:57\tepoch 0\tbatch 379\tloss 3.5051844120025635\n",
      "22:02:04\tepoch 0\tbatch 380\tloss 3.6250827312469482\n",
      "22:02:11\tepoch 0\tbatch 381\tloss 3.8259716033935547\n",
      "22:02:17\tepoch 0\tbatch 382\tloss 3.298604726791382\n",
      "22:02:24\tepoch 0\tbatch 383\tloss 3.623302936553955\n",
      "22:02:32\tepoch 0\tbatch 384\tloss 3.570939779281616\n",
      "22:02:39\tepoch 0\tbatch 385\tloss 3.703043222427368\n",
      "22:02:45\tepoch 0\tbatch 386\tloss 3.6490204334259033\n",
      "22:02:52\tepoch 0\tbatch 387\tloss 3.297211170196533\n",
      "22:02:59\tepoch 0\tbatch 388\t"
     ]
    }
   ],
   "source": [
    "train(3, dl, ie, cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, captions, lengths = next(iter(dl))\n",
    "features = ie.forward(images)\n",
    "generated = cg.forward(features, captions, lengths)\n",
    "sentences = cg.to_sentence(generated, lengths)\n",
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
